!/*****************************************************************************/
! *
! *  Elmer, A Finite Element Software for Multiphysical Problems
! *
! *  Copyright 1st April 1995 - , CSC - Scientific Computing Ltd., Finland
! * 
! *  This program is free software; you can redistribute it and/or
! *  modify it under the terms of the GNU General Public License
! *  as published by the Free Software Foundation; either version 2
! *  of the License, or (at your option) any later version.
! * 
! *  This program is distributed in the hope that it will be useful,
! *  but WITHOUT ANY WARRANTY; without even the implied warranty of
! *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
! *  GNU General Public License for more details.
! *
! *  You should have received a copy of the GNU General Public License
! *  along with this program (in file fem/GPL-2); if not, write to the 
! *  Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, 
! *  Boston, MA 02110-1301, USA.
! *
! *****************************************************************************/
!
!/******************************************************************************
! *
! *  These routines are for parallel version of ELMER solver.
! *  Subroutines for MPI-communication
! *
! ******************************************************************************
! *
! *  Authors: Jouni Malinen, Juha Ruokolainen
! *  Email:   Juha.Ruokolainen@csc.fi
! *  Web:     http://www.csc.fi/elmer
! *  Address: CSC - Scientific Computing Ltd.
! *           Keilaranta 14
! *           02101 Espoo, Finland 
! *
! *  Original Date: 2000
! *
! *****************************************************************************/
! *
! * $Id: SParIterComm.src,v 1.34 2007/04/18 11:57:58 jpr Exp $
! *****************************************************************************

#include "huti_fdefs.h"

MODULE SParIterComm

  USE Types
  USE SParIterGlobals

  IMPLICIT NONE

  INCLUDE "mpif.h"

CONTAINS

!-----------------------------------------------------------------
  SUBROUTINE CheckBuffer( n )
!-----------------------------------------------------------------
     INTEGER :: n, i, sz, ierr
     INTEGER*1, ALLOCATABLE :: SendBuffer(:)

     LOGICAL :: isfine

     SAVE SendBuffer

     sz = MAX( n, 2**20 )

     isfine = ALLOCATED(SendBuffer)
     IF ( isfine ) isfine = 4*sz <= SIZE(SendBuffer)
     IF ( isfine ) RETURN

     IF ( ALLOCATED(SendBuffer) ) THEN
        i = SIZE( SendBuffer )
        CALL MPI_BUFFER_DETACH( SendBuffer, i, ierr )
        DEALLOCATE( SendBuffer )
     END IF
     ALLOCATE( SendBuffer( 4*sz ) )
     CALL MPI_BUFFER_ATTACH( SendBuffer, 4*sz, ierr )
!-----------------------------------------------------------------
   END SUBROUTINE CheckBuffer
!-----------------------------------------------------------------


  !********************************************************************
  !********************************************************************
  !
  ! Initialize parallel execution environment
  !
   
!-----------------------------------------------------------------------
   FUNCTION ParCommInit( ) RESULT ( ParallelEnv ) 
!-----------------------------------------------------------------------
     TYPE (ParEnv_t), POINTER :: ParallelEnv

    ! Local variables

    INTEGER :: ierr

    !******************************************************************

    ParallelEnv => ParEnv

    ParEnv % MyPE = 0
    ParEnv % PEs  = 1

    ierr = 0
    CALL MPI_INIT( ierr )
    IF ( ierr /= 0 ) RETURN

    CALL MPI_COMM_SIZE( MPI_COMM_WORLD, ParEnv % PEs, ierr )
    IF ( ierr /= 0 .OR. ParEnv % PEs <= 1 ) THEN
       CALL MPI_Finalize( ierr )
    ELSE
       CALL MPI_COMM_RANK( MPI_COMM_WORLD, ParEnv % MyPE, ierr )

       WRITE( Message, * ) 'Initialize: ', ParEnv % PEs, ParEnv % MyPE
       CALL Info( 'ParCommInit', Message, Level=5 )
    
       IF ( ierr /= 0 ) THEN
          WRITE( Message, * ) 'MPI Initialization failed ! (ierr=', ierr, ')'
          CALL Fatal( 'ParCommInit', Message )
       END IF

       Parenv % NumOfNeighbours = 0
       ParEnv % Initialized = .TRUE.
    END IF
!-----------------------------------------------------------------------
  END FUNCTION ParCommInit
!-----------------------------------------------------------------------


!********************************************************************
!********************************************************************
! Initialize parallel execution environment
!-----------------------------------------------------------------------
  SUBROUTINE ParEnvInit( SPMatrix, ParallelInfo, SourceMatrix )
!-----------------------------------------------------------------------

    TYPE(SparIterSolverGlobalD_t) :: SPMatrix
    TYPE (ParallelInfo_t) :: ParallelInfo
    TYPE(Matrix_t) :: SourceMatrix
!-----------------------------------------------------------------------
    CALL FindActivePEs( ParallelInfo, SourceMatrix )
    SPMatrix % ParEnv = ParEnv
!-----------------------------------------------------------------------
  END SUBROUTINE ParEnvInit
!-----------------------------------------------------------------------


!-----------------------------------------------------------------------
  SUBROUTINE SParIterActive(L)
!-----------------------------------------------------------------------
    LOGICAL :: L
    INTEGER :: ierr
!-----------------------------------------------------------------------
    LOGICAL, ALLOCATABLE :: Active(:)
    ALLOCATE( Active(ParEnv % PEs) )

    IF ( .NOT. ASSOCIATED(ParEnv % Active) ) &
       ALLOCATE( ParEnv % Active(ParEnv % PEs) )

    ParEnv % Active = .FALSE.
    Active = .FALSE.
    Active(ParEnv % MYPe+1) = L
    CALL MPI_ALLREDUCE(Active,ParEnv % Active,ParEnv % PEs, &
         MPI_LOGICAL,MPI_LOR,MPI_COMM_WORLD,ierr)
    DEALLOCATE( Active )
!-----------------------------------------------------------------------
  END SUBROUTINE SParIterActive
!-----------------------------------------------------------------------


!********************************************************************
! Find active PEs using ParallelInfo % NeighbourList
!-----------------------------------------------------------------------
  SUBROUTINE FindActivePEs( ParallelInfo, SourceMatrix )
!-----------------------------------------------------------------------
    TYPE(Matrix_t) :: SourceMatrix
    TYPE(ParallelInfo_t) :: ParallelInfo
!-----------------------------------------------------------------------
    ! Local variables
    INTEGER, DIMENSION(MPI_STATUS_SIZE) :: status
    
    INTEGER :: i, j, k, sz, ierr, proc, MinActive
    INTEGER, ALLOCATABLE :: Active(:)
    LOGICAL :: L
    LOGICAL, ALLOCATABLE :: NeighAll(:,:)
    REAL(KIND=dp) :: RealTime, tstart, tend
    !******************************************************************

    CALL CheckBuffer( ParEnv % PEs**2 + MPI_BSEND_OVERHEAD )

    IF ( .NOT. ASSOCIATED(ParEnv % Active) ) THEN
      ALLOCATE( ParEnv % Active(ParEnv % PEs) )
      ParEnv % Active = .TRUE.
    END IF

    ALLOCATE( ParEnv % IsNeighbour(ParEnv % PEs) )
    ParEnv % NumOfNeighbours = 0
    ParEnv % IsNeighbour(:)  = .FALSE.

    !------------------------------------------------------------------
    ! Count the number of real neighbours for this partition
    !------------------------------------------------------------------
    DO i=1,SourceMatrix % NumberOfRows
       k = SourceMatrix % INVPerm(i)
       IF ( SIZE(ParallelInfo % NeighbourList(k) % Neighbours)>1 ) THEN
          DO j=1,SIZE(ParallelInfo % NeighbourList(k) % Neighbours)
             proc = ParallelInfo % NeighbourList(k) % Neighbours(j)
             IF ( ParEnv % Active(proc+1).AND.proc/=ParEnv % MYpe) &
                ParEnv % IsNeighbour(proc+1) = .TRUE.
          END DO
       END IF
    END DO

    !------------------------------------------------------------------
    ! Scan active procs using neighbourlists in GlobalDOFs
    ! TODO: wont work for disconnected areas....
    !------------------------------------------------------------------
    ! initialize with first order information
#if 0
    ParEnv % Active = ParEnv % IsNeighbour
    ParEnv % Active(parenv % myPE+1) = .TRUE.

!   ! This many exchanges are needed in the worst case for neighbouring 
!   ! information to propagate.
    ALLOCATE( Active(ParEnv % PEs) )
    DO k = 1, ParEnv % PEs
       Active = 0
       DO i=1,ParEnv % PEs
          IF ( ParEnv % Active(i) ) Active(i) = 1
       END DO

       DO i=1,ParEnv % PEs
          IF ( ParEnv % IsNeighbour(i) ) THEN
             CALL MPI_BSEND( Active, ParEnv % PEs, MPI_INTEGER, i-1, &
                  k, MPI_COMM_WORLD, ierr )
          END IF
       END DO

       DO i=1,ParEnv % PEs
          IF ( ParEnv % IsNeighbour(i) ) THEN
             CALL MPI_RECV( Active, ParEnv % PEs, MPI_INTEGER, i-1, &
                  k, MPI_COMM_WORLD, status, ierr )
             ParEnv % Active = ParEnv % Active .OR. Active /= 0
          END IF
       END DO
    END DO
#endif

    !------------------------------------------------------------------
    ! Sync all neighbouring information
    !------------------------------------------------------------------
    ALLOCATE( Active(ParEnv % PEs) )
    DO MinActive=0,ParEnv % PEs-1
      IF ( ParEnv % Active(MinActive+1) ) EXIT
    END DO
    Active = -1
    j = 0
    DO i=1,ParEnv % PEs
      IF ( ParEnv % IsNeighbour(i) ) THEN
         j = j + 1
         Active(j) = i-1
      END IF
    END DO

    IF (Parenv % myPE /= MinActive ) THEN
      CALL MPI_BSEND( j, 1, MPI_INTEGER, MinActive, &
                800, MPI_COMM_WORLD, ierr )
      IF ( j>0 ) THEN
        CALL MPI_BSEND( Active, j, MPI_INTEGER, MinActive, &
                 801, MPI_COMM_WORLD, ierr )
      END IF

      CALL MPI_RECV( j, 1, MPI_INTEGER, MinActive, &
             802, MPI_COMM_WORLD, status, ierr )
      IF ( j>0 ) THEN
        CALL MPI_RECV( Active, j, MPI_INTEGER, MinActive, &
               803, MPI_COMM_WORLD, status, ierr )
        DO i=1,j
          ParEnv % IsNeighbour(Active(i)+1) = .TRUE.
        END DO
      END IF
    ELSE
      ALLOCATE( NeighAll(ParEnv % PEs, ParEnv % PEs) )
      NeighAll=.FALSE.
      DO k=1,j
        NeighAll(MinActive+1,Active(j)+1) = .TRUE.
        NeighAll(Active(j)+1,MinActive+1) = .TRUE.
      END DO

      DO i=1,COUNT(ParEnv % Active)-1
        CALL MPI_RECV( j, 1, MPI_INTEGER, MPI_ANY_SOURCE, &
              800, MPI_COMM_WORLD, status, ierr )
        IF ( j>0 ) THEN
          proc = status(MPI_SOURCE)
          CALL MPI_RECV( Active, j, MPI_INTEGER, proc, &
              801, MPI_COMM_WORLD, status, ierr )
          DO k=1,j
            NeighAll(Active(k)+1,proc+1) = .TRUE.
            NeighAll(proc+1,Active(k)+1) = .TRUE.
            IF (Active(k)==MinActive) ParEnv % IsNeighbour(proc+1) = .TRUE.
          END DO
        END IF
      END DO

      DO i=1,Parenv % PEs
        IF ( ParEnv % Active(i) .AND. i-1/=MinActive ) THEN
          j = 0
          DO k=1,ParEnv % PEs
            IF ( ParEnv % Active(k) .AND. NeighAll(i,k) ) THEN
              j = j + 1
              Active(j) = k-1
            END IF
          END DO
          CALL MPI_BSEND( j, 1, MPI_INTEGER, i-1, &
                 802, MPI_COMM_WORLD, ierr )
          IF ( j>0 ) THEN
            CALL MPI_BSEND( Active, j, MPI_INTEGER, i-1, &
                   803, MPI_COMM_WORLD, ierr )
          END IF
        END IF
      END DO
      DEALLOCATE( NeighAll )
    END IF
    ParEnv % isNeighbour(ParEnv % myPE+1) = .FALSE.
    ParEnv % NumOfNeighbours = COUNT(ParEnv % isNeighbour)

    ! make sure that the owner of a matrix dof is one which
    ! has it active:
    ! -----------------------------------------------------
    DO i=1,SIZE(SourceMatrix % Perm)
      IF ( ParallelInfo % Interface(i) ) THEN

        sz = SIZE(ParallelInfo % NeighbourList(i) % Neighbours)
        DO j=1,sz
          k = ParallelInfo % NeighbourList(i) % Neighbours(j)
          IF ( k==ParEnv % Mype ) THEN
            Active(j) = SourceMatrix % Perm(i)
            EXIT
          END IF
        END DO

        DO j=1,sz
          k = ParallelInfo % NeighbourList(i) % Neighbours(j)
          IF ( ParEnv % Mype/=k .AND. ParEnv % Active(k+1) ) THEN
            CALL MPI_BSEND( SourceMatrix % Perm(i), 1, MPI_INTEGER, &
                  k, 100, MPI_COMM_WORLD, ierr)
          END IF
        END DO

        DO j=1,sz
          k = ParallelInfo % NeighbourList(i) % Neighbours(j)
          IF ( .NOT. ParEnv % Active(k+1) ) THEN
            Active(j) = 0
          ELSE IF ( ParEnv % Mype/=k ) THEN
            CALL MPI_RECV( Active(j), 1, MPI_INTEGER, k, 100,  &
                  MPI_COMM_WORLD, status, ierr)
          END IF
        END DO

        DO j=1,sz
          IF ( Active(j)/=0 ) THEN
            k = ParallelInfo % NeighbourList(i) % Neighbours(j)
            ParallelInfo % NeighbourList(i) % Neighbours(j) = &
               ParallelInfo % NeighbourList(j) % Neighbours(1)
            ParallelInfo % NeighbourList(i) % Neighbours(1) = k
            EXIT
          END IF
        END DO
      END IF
    END DO

    DEALLOCATE( Active )
!-----------------------------------------------------------------------
  END SUBROUTINE FindActivePEs
!-----------------------------------------------------------------------


!***********************************************************************
! Try to agree about global numbering of nodes among active
! processes.
!-----------------------------------------------------------------------

!-----------------------------------------------------------------------
  SUBROUTINE AddToCommonList( list, ENTRY )
!-----------------------------------------------------------------------
! Helper subroutine for SParIterGlobalNumbering
! Adds integer Entry to integer pointer List(:)
!-----------------------------------------------------------------------
    IMPLICIT NONE
    INTEGER, POINTER :: list(:)
    INTEGER :: ENTRY
!-----------------------------------------------------------------------
    INTEGER, POINTER :: ptmp(:)
    INTEGER :: itmp
!-----------------------------------------------------------------------
    IF( ASSOCIATED(list)) THEN
       itmp = SIZE(list)
       ALLOCATE(ptmp(itmp+1))
       ptmp(1:itmp) = list
       ptmp(itmp+1) = ENTRY
       DEALLOCATE(list)
       list => ptmp
    ELSE
       ALLOCATE(ptmp(1))
       ptmp(1) = ENTRY
       list => ptmp
    END IF
!-----------------------------------------------------------------------
  END SUBROUTINE AddToCommonList
!-----------------------------------------------------------------------  


!-----------------------------------------------------------------------  
   SUBROUTINE SParGlobalNumbering( Mesh, NewNodeCnt, &
            OldIntCnts, OldIntArray, Reorder )
!-----------------------------------------------------------------------
    USE GeneralUtils
!-----------------------------------------------------------------------
     TYPE(Mesh_t) :: Mesh
     INTEGER, TARGET :: NewNodeCnt, OldIntArray(:), &
               OldIntCnts(:), Reorder(:)
!-----------------------------------------------------------------------
     INTEGER, DIMENSION(MPI_STATUS_SIZE) :: status
     INTEGER :: ierr

     INTEGER, POINTER :: oldnodes(:), oldnodes2(:), newnodes(:), &
          newnodes2(:), parentnodes(:,:), tosend(:), toreceive(:), &
          list1(:), list2(:), commonlist(:)
     LOGICAL :: Found, AllReceived

     TYPE Node_t
        INTEGER, POINTER :: ElementIndexes(:)
     END TYPE Node_t
     TYPE(Node_t), POINTER :: Node(:)

     TYPE Request_t
        INTEGER, POINTER :: DATA(:)
     END TYPE Request_t
     TYPE(Request_t), POINTER :: Request1(:), Request2(:)

     TYPE(Element_t), POINTER :: Element

     INTEGER, PARAMETER :: MaxIterates = 10
     INTEGER :: i,j,k,l,m,mm,n,nn,MinProc,MaxLcl,MaxGlb,InterfaceNodes
     INTEGER :: LIndex(100), IntN, Gindex, k1, k2, Iterate, p, q
     INTEGER :: DataSize, TmpArray(3)
     INTEGER, POINTER :: IntArray(:),IntCnts(:),GIndices(:),Gorder(:)
!-----------------------------------------------------------------------
     CALL MPI_BARRIER( MPI_COMM_WORLD, ierr )

     Iterate = 0
! XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
! YOU, YES YOU  DO SOMETHING ABOUT THIS 
!     The size of the biggest pack to send is:
!     3*sizeof(int)*Number of new interface nodes
     CALL CheckBuffer(10000000)
! XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
!
!    find least numbered of the active procs:
!    ----------------------------------------
     DO MinProc = 1,ParEnv % PEs
        IF ( ParEnv % Active( MinProc ) ) EXIT
     END DO
!
!    Our maximum global node index:
!    -------------------------------
     MaxLcl = MAXVAL( Mesh % ParallelInfo % GlobalDOFs )
     MaxGlb = MaxLcl
     n = Mesh % Nodes % NumberOfNodes - NewNodeCnt + 1

!    Allocate space for local tables:
!    --------------------------------
     ALLOCATE( newnodes( ParEnv % PEs ), &
          newnodes2( ParEnv % PEs ), &
          oldnodes( ParEnv % PEs ), &
          oldnodes2( ParEnv % PEs ), &
          tosend( ParEnv % PEs ), &
          toreceive( ParEnv % PEs ), &
          parentnodes( Mesh % Nodes % NumberOfNodes,2 ) )
     
     newnodes    = 0
     newnodes2   = 0
     oldnodes    = 0
     oldnodes2   = 0
     tosend      = 0
     toreceive   = 0
     parentnodes = 0

     ALLOCATE( Request1( ParEnv % PEs ) )
     ALLOCATE( Request2( ParEnv % PEs ) )
     DO i = 1, ParEnv % PEs
        Request1(i) % DATA => NULL()
        Request2(i) % DATA => NULL()
     END DO
     !
     ! Prepare the inverse connection table for nodes and elements:
     !-------------------------------------------------------------
     ALLOCATE( Node( Mesh % Nodes % NumberOfNodes ) )
     DO i = 1,Mesh % Nodes % NumberOfNodes
        Node(i) % ElementIndexes => NULL()
     END DO
     
     DO i = 1,Mesh % NumberOfBulkElements
        Element => Mesh % Elements(i)
        DO j = 1,SIZE( Element % NodeIndexes )
           k = Element % NodeIndexes(j)
           CALL AddToCommonList( Node(k) % ElementIndexes, i )
        END DO
     END DO
     !
     ! Test write to a ep-file (please don't delete these lines, they might prove useful... :)
     !-----------------------------------------------------------------------------------------
     !PRINT *,'PE:',ParEnv % MyPE,'write ep...'
     !j = 10+ParEnv%MyPE
     !OPEN(unit=j)
     !WRITE(j,*) Mesh % Nodes % NumberOfNodes, &
     !     Mesh % NumberOfBulkElements, 1, 1, 'scalar: interface'
     !DO i = 1,Mesh % Nodes % NumberOfNodes
     !   WRITE(j,*) Mesh % Nodes % x(i), Mesh % Nodes % y(i), Mesh % Nodes % z(i)
     !END DO
     !DO i = 1, mesh % numberofbulkelements
     !   WRITE(j,*) 'body1 504', Mesh % Elements(i) % NodeIndexes-1
     !END DO
     !DO i = 1,Mesh % nodes % numberOfnodes
     !   IF( Mesh %ParallelInfo % INTERFACE(i) ) THEN
     !      WRITE(j,*) 1
     !   ELSE
     !     WRITE(j,*) 0
     !   END IF
     !END DO
     !CLOSE(j)

     !
     ! Find neighbours and parent nodes for all new interface nodes:
     ! =============================================================
     !
     ! Loop over all new nodes:
     !--------------------------
     DO i = n, Mesh % Nodes % NumberOfNodes
        IF( .NOT. Mesh % ParallelInfo % INTERFACE(i) ) CYCLE
        !
        ! This is an interface node:
        !---------------------------
        list1 => NULL() ! list of PEs related to parent node 1
        list2 => NULL() ! list of PEs related to parent node 2
        commonlist => NULL() ! intersection of list1 and list2
        !
        ! Go through all new elements connected to node i:
        !--------------------------------------------------
        DO j = 1, SIZE( Node(i) % ElementIndexes )
           Element => Mesh % Elements( Node(i) % ElementIndexes(j) )
           !
           ! Loop over all nodes of element j:
           !----------------------------------
           DO k = 1,SIZE( Element % NodeIndexes )
              !
              ! Search for parent nodes l that generated node i:
              !--------------------------------------------------
              l = Element % NodeIndexes(k)
              IF( l >= n ) CYCLE ! parents have local number < n
              IF( .NOT. Mesh % ParallelInfo % INTERFACE(l) ) CYCLE
              IF( ANY( parentnodes(i,:)==l) ) CYCLE ! already found
              !
              ! Construct the parent table:
              !----------------------------
              IF( parentnodes(i,1)==0 ) THEN
                 parentnodes(i,1) = l 
                 ! This is the list of PEs sharing parent node 1:
                 list1 => Mesh % ParallelInfo % NeighbourList(l) % Neighbours
              ELSE
                 parentnodes(i,2) = l 
                 ! This is the list of PEs sharing parent node 2:
                 list2 => Mesh % ParallelInfo % NeighbourList(l) % Neighbours
              END IF         
           END DO
        END DO
        !
        ! Determine the intersection of the PE-lists:
        !--------------------------------------------
        IF( ASSOCIATED(list1) .AND. ASSOCIATED(list2) ) THEN
           IF(ASSOCIATED(commonlist)) DEALLOCATE(commonlist)
           commonlist => NULL()
           DO p = 1,SIZE(list1)
              DO q = 1,SIZE(list2)
                 IF( list1(p) == list2(q)) &
                      CALL AddToCommonList( commonlist, list1(p) ) 
              END DO
           END DO
           !
           ! Now, we should have a list of PEs common to the parents:
           !----------------------------------------------------------
           IF( .NOT.ASSOCIATED(commonlist) ) CYCLE
           !
           ! If everything went ok and the mesh data was unique, there
           ! are only two PEs common to both parents. If not, thats bad:
           !------------------------------------------------------------
           !IF( SIZE(commonlist) > 2 ) THEN
           !   WRITE(*,'(A,I4,A,I6,A,2I6,A,2I6,A,10I4)') &
           !        'SParGlobalNumbering: PE:', ParEnv % MyPE+1, &
           !        ' Unable to determine owner for node(loc):', i, &
           !        ' Data not unique. Masters(loc):', parentnodes(i,:), &
           !        ' (glob):', Mesh % ParallelInfo % GlobalDOFs(parentnodes(i,:)), &
           !        ' Commonlist:', commonlist+1
           !END IF
           !
           ! Node i is given to the smallest numbered PE common to the parents:
           !-------------------------------------------------------------------
           ALLOCATE( gindices( SIZE(commonlist)))
           gindices = commonlist
           DEALLOCATE( commonlist )
           CALL SORT( SIZE(gindices), gindices )
           IF( ASSOCIATED( Mesh % ParallelInfo % NeighbourList(i) % Neighbours ) ) &
                DEALLOCATE( Mesh % ParallelInfo % NeighbourList(i) % Neighbours )
           Mesh % ParallelInfo % NeighbourList(i) % Neighbours => Gindices
        ELSE
           !
           ! Either list1 or list2 is empty. Thats really bad:
           !---------------------------------------------------
           WRITE(*,'(A,I4,A,I6)') 'SParIterGlobalNumbering: PE:', ParEnv % MyPE+1, &
                ' Could not determine owner for node(loc)=', i
           CALL MPI_FINALIZE( ierr )
        END IF
        !
        ! Finalize by sorting the parent table:
        !---------------------------------------
        CALL Sort( 2, parentnodes(i,:) )
     END DO

!************************************************************************************
10   CONTINUE
     !
     ! Iteratively update and correct the neighbour-lists and number the new nodes:
     !==============================================================================
     !
     ! Begin iteration (if the mesh was good, then only one round is needed):
     !-----------------------------------------------------------------------
     DO i = 1, ParEnv % PEs
        IF( ASSOCIATED( Request1(i) % DATA ) ) DEALLOCATE( Request1(i) % DATA )
        IF( ASSOCIATED( Request2(i) % DATA ) ) DEALLOCATE( Request2(i) % DATA )
        Request1(i) % DATA => NULL() ! buffer for data exchange
        Request2(i) % DATA => NULL() ! buffer for data exchange
     END DO
     !
     ! Count the current situation in what comes to nodes owned by us:
     !----------------------------------------------------------------
     oldnodes = 0
     newnodes = 0
     j = ParEnv % MyPE
     DO i = 1, Mesh % Nodes % NumberOfNodes
        k = Mesh % ParallelInfo % NeighbourList(i) % Neighbours(1)
        IF( k /= j ) CYCLE
        IF( Mesh % ParallelInfo % GlobalDOFs(i)  > 0 ) THEN
           oldnodes(j+1) = oldnodes(j+1)+1
        ELSE
           newnodes(j+1) = newnodes(j+1)+1
        END IF
     END DO
     !
     ! Distribute the knowledge about ownerships (here, assumig all active):
     !----------------------------------------------------------------------
     oldnodes2 = 0
     CALL MPI_ALLREDUCE( oldnodes, oldnodes2, ParEnv % PEs, &   ! <- fix 
          MPI_INTEGER, MPI_SUM, MPI_COMM_WORLD, ierr )

     newnodes2 = 0
     CALL MPI_ALLREDUCE( newnodes, newnodes2, ParEnv % PEs, &   ! <- fix 
          MPI_INTEGER, MPI_SUM, MPI_COMM_WORLD, ierr )
     !
     ! Number the new nodes:
     !----------------------
     j = ParEnv % MyPE
     ! Start numbering from index k:
     k = SUM( oldnodes2 ) + SUM( newnodes2(1:j) ) + 1
     DO i = 1, Mesh % Nodes % NumberOfNodes
        l = Mesh % ParallelInfo % NeighbourList(i) % Neighbours(1)
        IF( l /= j ) CYCLE
        IF( Mesh % ParallelInfo % GlobalDOFs(i) == 0 ) THEN
           Mesh % ParallelInfo % GlobalDOFs(i) = k
           k = k+1
        END IF
     END DO
     !
     ! Some new interface nodes may still lack a global number, if
     ! we are not the owner of these nodes. Hence, we'll have to make
     ! a query to all neighbour PEs to find these numbers.
     !===============================================================
     !
     ! Determine the size of data to exchange between neighbours:
     !-----------------------------------------------------------
     tosend = 0
     toreceive = 0
     DO i = n, Mesh % Nodes % NumberOfNodes
        IF( Mesh % ParallelInfo % INTERFACE(i) ) THEN
           j = Mesh % ParallelInfo % Neighbourlist(i) % Neighbours(1)
           IF( j /= ParEnv % MyPE ) THEN
              toreceive(j+1) = toreceive(j+1)+1
           ELSE
              DO j = 2,SIZE(Mesh % ParallelInfo % NeighbourList(i) % Neighbours )
                 k = Mesh % ParallelInfo % NeighbourList(i) % Neighbours(j)
                 tosend(k+1) = tosend(k+1)+1
              END DO
           END IF
        END IF
     END DO
     !
     ! Distribute the interface data:
     !--------------------------------
     DO i = 1, ParEnv % PEs
        IF( i-1 == ParEnv % MyPE ) CYCLE
        IF( .NOT. ParEnv % IsNeighbour(i) ) CYCLE

        CALL MPI_SEND( tosend(i), 1, MPI_INTEGER, i-1, 100, MPI_COMM_WORLD, ierr)
        IF( tosend(i) < 1 ) CYCLE

        DataSize = 3*tosend(i)
        ALLOCATE( gindices(DataSize) )
        k = 1
        DO l = n, Mesh % Nodes % NumberOfNodes
           IF( .NOT.( Mesh % ParallelInfo % INTERFACE(l) ) ) CYCLE
           m = Mesh % ParallelInfo % NeighbourList(l) % Neighbours(1)
           IF( m /= ParEnv % MyPE ) CYCLE

           DO m = 2,SIZE( Mesh % ParallelInfo % NeighbourList(l) % Neighbours )
              mm = Mesh % ParallelInfo % NeighbourList(l) % Neighbours(m)
              IF( mm+1 .NE. i ) CYCLE
              gindices(k) = Mesh % ParallelInfo % GlobalDOFs(l)
              gindices(k+1) = Mesh % ParallelInfo % GlobalDOFs(parentnodes(l,1))
              gindices(k+2) = Mesh % ParallelInfo % GlobalDOFs(parentnodes(l,2))
              k = k+3
           END DO
        END DO

        CALL MPI_BSEND( gindices, DataSize, MPI_INTEGER, i-1, 400, MPI_COMM_WORLD, ierr )
        
        DEALLOCATE( gindices )
     END DO
     !
     ! Retrieve missing interface node numbers from our neighbours:
     !--------------------------------------------------------------
     DO i = 1, ParEnv % PEs
        IF( i-1 == ParEnv % MyPE ) CYCLE
        IF( .NOT. ParEnv % IsNeighbour(i) ) CYCLE

        CALL MPI_RECV( DataSize, 1, MPI_INTEGER, i-1, 100, MPI_COMM_WORLD, status, ierr )
        !IF( DataSize /= toreceive(i) ) THEN
        !   WRITE(*,'(A,I4,A,I4,A,I5,A,I5)') 'SParGlobalNumbering: PE:',ParEnv % MyPE+1, &
        !        ' Disagreement about data size with PE:', i, ' Expecting:',toreceive(i), &
        !        ' Actually got:', DataSize
        !END IF
        IF( DataSize < 1 ) CYCLE

        ALLOCATE( gindices(3*DataSize) ) ! work space
        CALL MPI_RECV( gindices, 3*DataSize, MPI_INTEGER, i-1, 400, MPI_COMM_WORLD, status, ierr )

        DO k = n, Mesh % Nodes % NumberOfnodes
           IF( .NOT. Mesh % ParallelInfo % INTERFACE(k) ) CYCLE
           IF( Mesh % ParallelInfo % GlobalDOFs(k) > 0 ) CYCLE
           
           DO l = 1, DataSize
              m = 3*l-2
              IF( Mesh % ParallelInfo % GlobalDOFs(parentnodes(k,1)) == gindices(m+1) .AND. &
                   Mesh % ParallelInfo % GlobalDOFs(parentnodes(k,2)) == gindices(m+2) ) THEN
                 Mesh % ParallelInfo % GlobalDOFs(k) = gindices(m) ! global DOF number
                 gindices(m) = 0 ! mark used
                 EXIT
              END IF
           END DO
        END DO
        !
        ! PROBLEM CASE 1: Received too much data
        ! =======================================
        ! It is possible that we actually got too much data, because one or more PEs
        ! ennareously think of us as their neighbour. In this case, we will have to
        ! inform these PEs about the situation and ask them to remove us from their
        ! neighbour lists:
        !----------------------------------------------------------------------------
        DO l=1,DataSize
           IF( gindices(3*l-2)>0 ) THEN
              !WRITE(*,'(A,I5,A,I8,A,2I8)') 'SParGlobalNumbering: PE',ParEnv % MyPE+1, &
              !     ' Got unexpected global data for node(glob):',gindices(3*l-2),&
              !     ' Parent nodes(glob):', gindices(3*l-1), gindices(3*l)
              !PRINT *,'Sending a request to',i,'to remove me from the neighnour list.'
              !
              ! Request PE=i to remove us from the neighbour list:
              !----------------------------------------------------
              CALL AddToCommonList( Request1(i) % DATA, gindices(3*l-2) )  ! i-node
              CALL AddToCommonList( Request1(i) % DATA, gindices(3*l-1) )  ! parent 1
              CALL AddToCommonList( Request1(i) % DATA, gindices(3*l-0) )  ! parent 2
           END IF
        END DO
        DEALLOCATE( gindices )
     END DO
     !
     ! PROBLEM CASE 2: Data not sufficient
     ! ===================================
     ! We should have received unique global dof-numbers from the owners of the
     ! corresponding interface nodes. If we did not, our own neighbour list is
     ! too long and we have a false idea about the owner. In this case, we will
     ! have to ask all possible neighbours to check if they have the node, and if
     ! they don't, ask them to return a request to be removed from our neighbour list:
     !---------------------------------------------------------------------------------
     DO i = n, Mesh % NumberOfNodes
        IF( Mesh % ParallelInfo % GlobalDOFs(i) < 1 ) THEN
           !WRITE(*,'(A,I5,A,I8,A,2I8)') 'SParGlobalNumbering: PE',ParEnv % MyPE+1, &
           !     ' Did not receive global number for node(loc):', i, ' Parents nodes(glob):', &
           !     Mesh % ParallelInfo % GlobalDOFs( parentnodes(i,:) )
           !PRINT *,'Have to inform the following PEs:', &
           !     Mesh % ParallelInfo % NeighbourList(i) % Neighbours +1 
           DO j = 1,SIZE( Mesh % ParallelInfo % NeighbourList(i) % Neighbours )
              k = Mesh % ParallelInfo % NeighbourList(i) % Neighbours(j)
              IF( k == ParEnv % MyPE ) CYCLE
              TmpArray(1) = Mesh % ParallelInfo % GlobalDOFs( i )
              TmpArray(2) = Mesh % ParallelInfo % GlobalDOFs( parentnodes(i,1) )
              TmpArray(3) = Mesh % ParallelInfo % GlobalDOFs( parentnodes(i,2) )
              CALL AddToCommonList( Request2(k+1) % DATA, TmpArray(1) )
              CALL AddToCommonList( Request2(k+1) % DATA, TmpArray(2) )
              CALL AddToCommonList( Request2(k+1) % DATA, TmpArray(3) )
           END DO
        END IF
     END DO
     !
     ! Send the query (CASE 2):
     !--------------------------
     DO i = 1, ParEnv % PEs              
        IF( i-1 == ParEnv % MyPE ) CYCLE
        IF( .NOT. ParEnv % IsNeighbour(i) ) CYCLE

        DataSize = 0
        IF( ASSOCIATED( Request2(i) % DATA ) ) DataSize = SIZE( Request2(i) % DATA )

        CALL MPI_SEND( DataSize, 1, MPI_INTEGER, i-1, 900, MPI_COMM_WORLD, ierr)
        IF( DataSize < 1 ) CYCLE

        CALL MPI_BSEND( Request2(i) % DATA, DataSize, MPI_INTEGER, i-1, 950, MPI_COMM_WORLD, ierr)
     END DO
     !
     ! Receive the query (CASE 2):
     !----------------------------
     DO i = 1, ParEnv % PEs
        IF( i-1 == ParEnv % MyPE ) CYCLE
        IF( .NOT. ParEnv % IsNeighbour(i) ) CYCLE

        CALL MPI_RECV( DataSize, 1, MPI_INTEGER, i-1, 900, MPI_COMM_WORLD, status, ierr )
        IF( DataSize < 1 ) CYCLE
        
        ALLOCATE( IntArray(DataSize) )
        CALL MPI_RECV( IntArray, DataSize, MPI_INTEGER, i-1, 950, MPI_COMM_WORLD, status, ierr )

        DO q = 1, DataSize/3
           TmpArray = IntArray(3*q-2:3*q)
           !
           !PRINT *,'SParGlobalNumbering: PE',ParEnv % MyPE+1,' received a query from PE',i, &
           !     ' which asks if Im the owner of parents(glob):', TmpArray(2), TmpArray(3)
           !
           ! Ok, lets see if we have the parents requested:
           !-----------------------------------------------
           mm = 0
           nn = 0
           DO j = n, Mesh % NumberOfNodes
              IF( .NOT. Mesh % ParallelInfo % INTERFACE(j)) CYCLE
              mm = 0
              nn = 0
              DO k = 1,SIZE( Node(j) % ElementIndexes )
                 l = Node(j) % ElementIndexes(k)
                 Element => Mesh % Elements(l)
                 IF( ANY(Mesh % ParallelInfo % GlobalDOFs(Element % NodeIndexes) == TmpArray(2)) ) mm = 1
                 IF( ANY(Mesh % ParallelInfo % GlobalDOFs(Element % NodeIndexes) == TmpArray(3)) ) nn = 1
              END DO
              IF( mm==1 .AND. nn==1 ) THEN
                 !PRINT *,'Ok, ive got it. node(loc):',j,'(glob):',mesh % parallelInfo % GlobalDOFs(j)
                 EXIT
              END IF
           END DO
           
           IF( .NOT.(mm==1 .AND. nn==1) ) THEN
              !PRINT *,'Nope, I dont have it. Sending back request to remove me from the neighbour list.'
              CALL AddToCommonList( Request1(i) % DATA, TmpArray(1) )
              CALL AddToCommonList( Request1(i) % DATA, TmpArray(2) )
              CALL AddToCommonList( Request1(i) % DATA, TmpArray(3) )
           END IF
        END DO
        DEALLOCATE( IntArray )
     END DO
     !
     ! Send the removal requests to all neighbours (CASE 1):
     !------------------------------------------------------
     DO i = 1, ParEnv % PEs              
        IF( i-1 == ParEnv % MyPE ) CYCLE
        IF( .NOT. ParEnv % IsNeighbour(i) ) CYCLE

        DataSize = 0
        IF( ASSOCIATED( Request1(i) % DATA ) ) DataSize = SIZE( Request1(i) % DATA )

        CALL MPI_SEND( DataSize, 1, MPI_INTEGER, i-1, 700, MPI_COMM_WORLD, ierr)
        IF( DataSize < 1 ) CYCLE

        CALL MPI_BSEND( Request1(i) % DATA, DataSize, MPI_INTEGER, i-1, 800, MPI_COMM_WORLD, ierr )
     END DO
     !
     ! Receive the removal requests and update the neighbour lists (CASE 1):
     !----------------------------------------------------------------------
     DO i = 1, ParEnv % PEs
        IF( i-1 == ParEnv % MyPE ) CYCLE
        IF( .NOT. ParEnv % IsNeighbour(i) ) CYCLE

        CALL MPI_RECV( DataSize, 1, MPI_INTEGER, i-1, 700, MPI_COMM_WORLD, status, ierr )
        IF( DataSize < 1 ) CYCLE
        
        ALLOCATE( IntArray(DataSize) )
        CALL MPI_RECV( IntArray, DataSize, MPI_INTEGER, i-1, 800, MPI_COMM_WORLD, status, ierr )
        
        mm = DataSize/3
        DO nn = 1,mm
           TmpArray = IntArray(3*nn-2:3*nn)
           !PRINT *,'SParGlobalNumbering: PE',ParEnv % MyPE+1,' got a message from PE',i, &
           !     ' which asks to be removed from the neighbour list of node(glob):', &
           !     TmpArray(1), 'Parents(glob):', TmpArray(2), TmpArray(3)
           DO j = n, Mesh % NumberOfNodes
              IF( Mesh % ParallelInfo % GlobalDOFs(j) == TmpArray(1)) THEN
                 ! Check also for parents to avoid screwing up anything:
                 IF( .NOT.( mesh % parallelinfo % globaldofs(parentnodes(j,1)) == TmpArray(2) .AND. &
                      mesh % parallelinfo % globaldofs(parentnodes(j,2)) == TmpArray(3) ) ) CYCLE
                 !PRINT *,'Ok, found it. Local number is', j
                 !PRINT *,'Neighbours were:', Mesh % ParallelInfo % NeighbourList(j) % Neighbours+1
                 !
                 ! Removing:
                 !----------
                 IF( ASSOCIATED(gorder)) DEALLOCATE( gorder )
                 gorder => NULL()
                 DO k = 1,SIZE( Mesh % ParallelInfo % NeighbourList(j) % Neighbours )
                    l = Mesh % ParallelInfo % NeighbourList(j) % Neighbours(k)
                    IF( l == i-1 ) CYCLE
                    CALL AddToCommonList( gorder, l )
                 END DO
                 DEALLOCATE( Mesh % ParallelInfo % NeighbourList(j) % Neighbours )
                 ALLOCATE( IntCnts( SIZE(gorder) ) )
                 IntCnts = gorder
                 DEALLOCATE( gorder )
                 Mesh % ParallelInfo % NeighbourList(j) % Neighbours => IntCnts
                 !PRINT *,'Updated neighbours:', Mesh % ParallelInfo % NeighbourList(j) % Neighbours+1
                 EXIT
              END IF
           END DO
        END DO
        DEALLOCATE( IntArray )
     END DO     
     !
     ! Make sure that all nodes have a global number:
     !-----------------------------------------------
     Found = .FALSE.
     DO i = 1,Mesh % NumberOfNodes
        IF( Mesh % ParallelInfo % GlobalDOFs(i) < 1 ) THEN
           Found = .TRUE.
           !WRITE(*,'(A,I4)') 'SParIterGlobalNumbering: PE:', ParEnv % MyPE+1, &
           !     ' No global number for node(loc):',i
           IF( ASSOCIATED( mesh % parallelinfo % neighbourlist(i) % neighbours ) ) THEN
              !PRINT *, 'Owners:', Mesh % ParallelInfo % NeighbourList(i) % Neighbours+1
           ELSE
              CALL AddToCommonList( Mesh % ParallelInfo % NeighbourList(i) % Neighbours, ParEnv % MyPE )
           END IF
        END IF

        ! Check also the neighbour lists:
        !--------------------------------
        IF( ASSOCIATED( Mesh % ParallelInfo % NeighbourList(i) % Neighbours ) ) THEN
           IF( SIZE(Mesh % ParallelInfo % NeighbourList(i) % Neighbours) < 1 ) THEN
              PRINT *,'PE, node: ***** This node is missing the owner ****',ParEnv % MyPE+1, i
           END IF
           
           IF (ANY(Mesh % ParallelInfo % NeighbourList(i) % Neighbours < 0 ) .OR. &
                ANY(Mesh % ParallelInfo % NeighbourList(i) % Neighbours > ParEnv % PEs-1 ) ) THEN
              PRINT *,'PE, node: ***** This node has a bad owner ****',ParEnv % MyPE+1, i
              PRINT *, Mesh % ParallelInfo % NeighbourList(i) % Neighbours
              Mesh % ParallelInfo % NeighbourList(i) % Neighbours(1) = ParEnv % MyPE              
           END IF
        ELSE
           PRINT *,'PE, node: ***** This node is missing the owner ****',ParEnv % MyPE+1, i
        END IF
     END DO

     oldnodes = 0 ! workspace
     IF( Found ) oldnodes( ParEnv % MyPE+1 ) = 1
     oldnodes2 = 0
     CALL MPI_ALLREDUCE( oldnodes, oldnodes2, ParEnv % PEs, MPI_INTEGER, MPI_SUM, MPI_COMM_WORLD, ierr )
     j = SUM( oldnodes2 )

     IF( j < 1 ) GOTO 20
     !IF( ParEnv % MyPE==0 ) &
     !     write(*,'(A,I6,A)') 'SParIterGlobalNumbering:  Numbering not consistent, re-iterating...'
     Iterate = Iterate+1
     IF(Iterate > MaxIterates ) THEN
        WRITE(*,'(A,I6,A)') 'SParIterGlobalNumbering: PE: ', ParEnv % MyPE+1,'Max iterations exceeded'
        CALL MPI_FINALIZE( MPI_COMM_WORLD, ierr )
     END IF
     DO i = n, Mesh % NumberOfNodes
        Mesh % ParallelInfo % GlobalDOFs(i) = 0
     END DO
     GOTO 10

20   CONTINUE
     IF( ParEnv % MyPE==0 ) &
          WRITE(*,'(A,I6,A)') 'SParIterGlobalNumbering:  Numbering seems consistent, advancing...'

     !-------------------------------------------------------------------
     ! Now, the numbering should be unique. It remains to sort the nodes.
     !-------------------------------------------------------------------
!
!    Sort our own nodes according to ascending
!    global order:
!    -----------------------------------------
     ALLOCATE( IntCnts(NewNodeCnt), Gorder(NewNodeCnt) )
     DO i=1,NewNodeCnt
        Gorder(i) = i
     END DO 

     CALL SortI( NewNodeCnt, Mesh % ParallelInfo % GlobalDOFs(n:), Gorder )

     DO i=1,NewNodeCnt
        IntCnts(Gorder(i)) = i
     END DO
!
!    Reorder will return the nodal reordering
!    to the caller:
!    ----------------------------------------
     Reorder(n:) = IntCnts + n - 1
!
!    Order the whole of the nodal structure
!    according to the changed order of the 
!    global numbers:
!    --------------------------------------
     DO i=1,NewNodeCnt
        k = Gorder(i)
        CALL SwapNodes( Mesh, i+n-1, k+n-1 )

        j = IntCnts(i)
        IntCnts(i) = IntCnts(k)
        IntCnts(k) = j

        Gorder(IntCnts(k)) = k
     END DO

     DEALLOCATE( IntCnts )

     ! Deallocate temp arrays:
     !------------------------
     DEALLOCATE( oldnodes, oldnodes2, newnodes, newnodes2, &
          parentnodes, tosend, toreceive )
     
     DO i = 1,Mesh % Nodes % NumberOfNodes
        DEALLOCATE( Node(i) % ElementIndexes )
     END DO
     DEALLOCATE( Node )

     DO i = 1, ParEnv % PEs
        IF( ASSOCIATED( Request1(i) % DATA ) ) DEALLOCATE( Request1(i) % DATA )
        IF( ASSOCIATED( Request2(i) % DATA ) ) DEALLOCATE( Request2(i) % DATA )
     END DO
     DEALLOCATE( Request1, Request2 )

     CALL MPI_BARRIER( MPI_COMM_WORLD, ierr )

     RETURN

!???????????????????????????????????????????????????????????????????????????????

!
!    Lowest numbered PE will compute the size of
!    the (old) global node array, and eventually
!    distribute the knowledge:
!
!    Others send their maxlcl to minproc
!    -------------------------------------------
     IF ( ParEnv % MyPE == MinProc-1 ) THEN
        j = 1 !?????
        DO i = MinProc+1, ParEnv % PEs
           IF ( ParEnv % Active(i) ) THEN
              CALL MPI_RECV( k, 1, MPI_INTEGER, i-1, &
                   10, MPI_COMM_WORLD, status, ierr )
              MaxGlb = MAX( MaxGlb, k )
           END IF
        END DO
     ELSE
        CALL MPI_BSEND( MaxLcl, 1, MPI_INTEGER, &
             MinProc-1, 10, MPI_COMM_WORLD, ierr )
     END IF

!print *,'(2) Id:',ParEnv % MyPE+1,'MaxGlb=',MaxGlb

!
!    Recieve new interface nodes from lower
!    numbered PEs, and check if they are
!    relevant to us:
!    ---------------------------------------

     DO i = MinProc, ParEnv % MyPE

!if( ParEnv % IsNeighbour(i) ) &
!     print *,'(3) Id:',ParEnv % MyPE+1,'Neighbour=',i

        IF ( .NOT. (ParEnv % Active(i) .AND. ParEnv % Isneighbour(i)) ) CYCLE
        
        CALL MPI_RECV( InterfaceNodes, 1, MPI_INTEGER, &
             i-1, 14, MPI_COMM_WORLD, status, ierr )

!print *,'(4) Id:',ParEnv % MyPE+1,'Neighbour=',i,'InterfaceNodes=',InterfaceNodes
        
        IF ( InterfaceNodes > 0 ) THEN
           ALLOCATE( GIndices(InterfaceNodes), IntCnts(InterfaceNodes) )
           
           CALL MPI_RECV( GIndices, InterfaceNodes, MPI_INTEGER, &
                i-1, 15, MPI_COMM_WORLD, status, ierr )

!print *,'(5) Id:', ParEnv % MyPE+1, 'Neighbour=',i,'GIndices=',GIndices
           
           CALL MPI_RECV( IntCnts, InterfaceNodes, MPI_INTEGER, &
                i-1, 16, MPI_COMM_WORLD, status, ierr )

!print *,'(6) Id:', ParEnv % MyPE+1, 'Neighbour=',i,'IntCnts=',IntCnts
           
           CALL MPI_RECV( k, 1, MPI_INTEGER, &
                i-1, 17, MPI_COMM_WORLD, status, ierr )

!print *,'(7) Id:', ParEnv % MyPE+1, 'Neighbour=',i,'k=',k
           
           ALLOCATE( IntArray(k) )
           
           CALL MPI_RECV( IntArray, SIZE(IntArray), MPI_INTEGER, &
                i-1, 18, MPI_COMM_WORLD, status, ierr )

!print *,'(8) Id:', ParEnv % MyPE+1, 'Neighbour=',i,'IntArray=',IntArray

!
!          Update our view of the global numbering
!          at the interface nodes:
!          ---------------------------------------
           l = 0
           DO j=1,InterfaceNodes
              Lindex = 0
              IntN = IntCnts(j)
              DO k=1,IntN
                 Lindex(k) = SearchNode( Mesh % ParallelInfo, IntArray(l+k), 1, n-1 )
              END DO

              IF ( ALL( Lindex(1:IntN) > 0 ) ) THEN
!
!                This node belongs to us as well well:
!                -------------------------------------
                 k2 = 0
                 k1 = 0
                 DO k=n,Mesh % Nodes % NumberOfNodes
                    IF ( .NOT.Mesh % ParallelInfo % INTERFACE(k) ) CYCLE

                    k1 = k1 + 1
                    IF ( IntN == OldIntCnts(k1) ) THEN
                       IF ( ALL( IntArray(l+1:l+IntN) == &
                                    OldIntArray(k2+1:k2+IntN)) ) THEN
                           Mesh % ParallelInfo % GlobalDOFs(k) = GIndices(j)
                           EXIT
                        END IF
                     END IF
                     k2 = k2 + OldIntCnts(k1)
                 END DO
              END IF
              l = l + IntN
           END DO
           DEALLOCATE( Gindices, IntCnts, IntArray )
        END IF
     END DO
!
!    Update the current numbering from the 
!    previous PE in line, this will make the
!    execution strictly serial. Maybe there
!    would be an easier way?
!    ---------------------------------------
     IF ( ParEnv % MyPE > MinProc-1 ) THEN
        DO i=ParEnv % MyPE, MinProc, -1
           IF ( ParEnv % Active(i) ) THEN
              CALL MPI_RECV( MaxGlb, 1, MPI_INTEGER, &
               i-1, 20, MPI_COMM_WORLD, status, ierr )

!print *,'(20) Id:',ParEnv % MyPE+1,'dest=',i,'MaxGlb=',MaxGlb

              EXIT
           END IF
        END DO
     END IF
!
!    Renumber our own new set of nodes:
!    ----------------------------------
     DO i=n,Mesh % Nodes % NumberOfNodes
        IF ( Mesh % ParallelInfo % GlobalDOFs(i) == 0 ) THEN
           MaxGlb = MaxGlb + 1
           Mesh % ParallelInfo % GlobalDOFs(i) = MaxGlb
        END IF
     END DO

!
!    Extract interface nodes:
!    ------------------------
     InterfaceNodes = COUNT( Mesh % ParallelInfo % INTERFACE(n:) )
     IF ( InterfaceNodes > 0 ) ALLOCATE( Gindices(InterfaceNodes) )

     InterfaceNodes = 0
     DO i=n,Mesh % Nodes % NumberOfNodes
        IF ( Mesh % ParallelInfo % INTERFACE(i) ) THEN
           InterfaceNodes = InterfaceNodes + 1
           Gindices(InterfaceNodes) = Mesh % ParallelInfo % GlobalDOFs(i)
        END IF
     END DO
!
!    Send new interface nodes to higher numbered PEs:
!    ------------------------------------------------
     DO i=ParEnv % MyPE+2,ParEnv % PEs
        IF ( .NOT. (ParEnv % Active(i) .AND. ParEnv % Isneighbour(i)) ) CYCLE

        CALL MPI_BSEND( InterfaceNodes, 1, MPI_INTEGER, &
               i-1, 14, MPI_COMM_WORLD, ierr )

        IF ( InterfaceNodes > 0 ) THEN
           CALL MPI_BSEND( GIndices, InterfaceNodes, MPI_INTEGER, &
                    i-1, 15, MPI_COMM_WORLD, ierr )

           CALL MPI_BSEND( OldIntCnts, InterfaceNodes, &
              MPI_INTEGER, i-1, 16, MPI_COMM_WORLD, ierr )

           CALL MPI_BSEND( SIZE(OldIntArray), 1, &
              MPI_INTEGER, i-1, 17, MPI_COMM_WORLD, ierr )

           CALL MPI_BSEND( OldIntArray, SIZE(OldIntArray), &
              MPI_INTEGER, i-1, 18, MPI_COMM_WORLD, ierr )

        END IF
     END DO

     DEALLOCATE( GIndices )
!
!    Send go singal to next PE in line...
!    ------------------------------------
     DO i = ParEnv % MyPE+2, ParEnv % PEs
        IF ( ParEnv % Active(i) ) THEN
           CALL MPI_BSEND( MaxGlb, 1, MPI_INTEGER, &
               i-1, 20, MPI_COMM_WORLD, ierr )
           EXIT
        END IF
     END DO
!
!    Sort our own nodes according to ascending
!    global order:
!    -----------------------------------------
     ALLOCATE( IntCnts(NewNodeCnt), Gorder(NewNodeCnt) )
     DO i=1,NewNodeCnt
        Gorder(i) = i
     END DO

     CALL SortI( NewNodeCnt, Mesh % ParallelInfo % GlobalDOFs(n:), Gorder )

     DO i=1,NewNodeCnt
        IntCnts(Gorder(i)) = i
     END DO
!
!    Reorder will return the nodal reordering
!    to the caller:
!    ----------------------------------------
     Reorder(n:) = IntCnts + n - 1
!
!    Order the whole of the nodal structure
!    according to the changed order of the 
!    global numbers:
!    --------------------------------------
     DO i=1,NewNodeCnt
        k = Gorder(i)
        CALL SwapNodes( Mesh, i+n-1, k+n-1 )

        j = IntCnts(i)
        IntCnts(i) = IntCnts(k)
        IntCnts(k) = j

        Gorder(IntCnts(k)) = k
     END DO

     DEALLOCATE( IntCnts )
!
!    Ok, now we have generated the global numbering
!    of nodes. We still need to distribute the
!    information which PEs share which of the new
!    interface nodes:
!    -----------------------------------------------
     InterfaceNodes = COUNT( Mesh % ParallelInfo % INTERFACE(n:) )
     ALLOCATE( GIndices( InterfaceNodes ) )
     j = 0
     DO i=n,Mesh % Nodes % NumberOfNodes
        IF ( Mesh % ParallelInfo % INTERFACE(i) ) THEN
           j = j + 1
           GIndices(j) = Mesh % ParallelInfo % GlobalDOFs(i)
           ALLOCATE( Mesh % ParallelInfo % NeighbourList(i) % Neighbours(ParEnv % PEs) )
           Mesh % ParallelInfo % NeighbourList(i) % Neighbours = -1
        END IF
     END DO

     DO i=MinProc,ParEnv % PEs
        IF ( ParEnv % MyPE == i-1 ) CYCLE
        IF ( ParEnv % Active(i) .AND. ParEnv % IsNeighbour(i) ) THEN
           CALL MPI_BSEND( InterfaceNodes, 1, &
              MPI_INTEGER, i-1, 30, MPI_COMM_WORLD, ierr )

           CALL MPI_BSEND( GIndices, InterfaceNodes, &
              MPI_INTEGER, i-1, 31, MPI_COMM_WORLD, ierr )
        END IF
     END DO

     DEALLOCATE( Gindices )

     ALLOCATE( IntCnts( Mesh % Nodes % NumberOfNodes ) )

     IntCnts = 0
     DO i=n,Mesh % Nodes % NumberOfNodes
        IF ( Mesh % ParallelInfo % INTERFACE(i) ) THEN
           IntCnts(i) = IntCnts(i) + 1
           Mesh % ParallelInfo % NeighbourList(i) % Neighbours(1) = ParEnv % MyPE
        END IF
     END DO

     DO i=MinProc,ParEnv % PEs
        IF ( ParEnv % MyPE == i-1 ) CYCLE
        IF ( ParEnv % Active(i) .AND. ParEnv % IsNeighbour(i) ) THEN
           CALL MPI_RECV( InterfaceNodes, 1, MPI_INTEGER, &
               i-1, 30, MPI_COMM_WORLD, status, ierr )

           ALLOCATE( GIndices(InterfaceNodes) )

           CALL MPI_RECV( GIndices, InterfaceNodes, MPI_INTEGER, &
               i-1, 31, MPI_COMM_WORLD, status, ierr )

           DO j=1,InterfaceNodes
              k = SearchNode( Mesh % ParallelInfo, Gindices(j), n )
              IF ( k <= 0 ) CYCLE
              IntCnts(k) = IntCnts(k) + 1
              Mesh % ParallelInfo % NeighbourList(k) % Neighbours(IntCnts(k)) = i-1
           END DO

           DEALLOCATE( GIndices )
        END IF
     END DO
!
!    Reallocate the nodal neighbour lists to
!    correct sizes:
!    ---------------------------------------
     DO i=n,Mesh % Nodes % NumberOfNodes
        IF ( Mesh % ParallelInfo % INTERFACE(i) ) THEN
           k = IntCnts(i)
           ALLOCATE( Gindices(k) ) ! just work space
           Gindices = Mesh % ParallelInfo % NeighbourList(i) % Neighbours(1:k)
           CALL Sort( k, Gindices )
           DEALLOCATE( Mesh % ParallelInfo % NeighbourList(i) % Neighbours )
           Mesh % ParallelInfo % NeighbourList(i) % Neighbours => Gindices
        END IF
     END DO
     
     DEALLOCATE( IntCnts )

! final test:
!     IF( ParEnv % MyPE == 3 ) THEN
!        DO i = 1, Mesh % Nodes % NumberOfNodes
!           PRINT *,'Local:',i, &
!                'Global:' ,Mesh % Parallelinfo % GlobalDOFs(i), &
!                'Interface:', Mesh % ParallelInfo % INTERFACE(i), &
!                'Neighbours:', Mesh % ParallelInfo % NeighbourList(i) % Neighbours + 1
!        END DO
!     END IF

PRINT *,'****OK:', parenv % mype+1
DO i = 1, mesh % nodes % numberofnodes
   PRINT *,'(+++)',parenv % mype+1, i, Mesh % ParallelInfo % GlobalDOFs(i), &
        Mesh % ParallelInfo % INTERFACE(i), Mesh % ParallelInfo % NeighbourList(i) % Neighbours
END DO


     CALL MPI_BARRIER( MPI_COMM_WORLD, ierr )

CONTAINS

!-----------------------------------------------------------------------
     SUBROUTINE SwapNodes( Mesh, i, k )
!-----------------------------------------------------------------------
        INTEGER :: i,k
        TYPE(Mesh_t) :: Mesh
!-----------------------------------------------------------------------
        REAL(KIND=dp) :: swapx,swapy,swapz
        LOGICAL :: swapi
        INTEGER, POINTER :: swapl(:)
!-----------------------------------------------------------------------
        swapx =  Mesh % Nodes % x(i)
        swapy =  Mesh % Nodes % y(i)
        swapz =  Mesh % Nodes % z(i)
        swapi =  Mesh % ParallelInfo % INTERFACE(i)
        swapl => Mesh % ParallelInfo % NeighbourList(i) % Neighbours
 
        Mesh % Nodes % x(i) = Mesh % Nodes % x(k)
        Mesh % Nodes % y(i) = Mesh % Nodes % y(k)
        Mesh % Nodes % z(i) = Mesh % Nodes % z(k)
        Mesh % ParallelInfo % INTERFACE(i) = Mesh % ParallelInfo % INTERFACE(k) 
        Mesh % ParallelInfo % NeighbourList(i) % Neighbours => &
                 Mesh % ParallelInfo % NeighbourList(k) % Neighbours

        Mesh % Nodes % x(k) = swapx
        Mesh % Nodes % y(k) = swapy
        Mesh % Nodes % z(k) = swapz
        Mesh % ParallelInfo % INTERFACE(k) = swapi
        Mesh % ParallelInfo % NeighbourList(k) % Neighbours => swapl
!-----------------------------------------------------------------------
     END SUBROUTINE SwapNodes
!-----------------------------------------------------------------------
   END SUBROUTINE SParGlobalNumbering
!-----------------------------------------------------------------------

!*********************************************************************
! Do global numbering for elements, edges and faces.
! (now only works for 1 dof edges .)
!---------------------------------------------------------------------
   SUBROUTINE InitialGlobalNumbering( Mesh, NumberOfDOFs, &
            OldIntCnts, OldIntArray, Reorder )
!---------------------------------------------------------------------
     TYPE(Mesh_t) :: Mesh
     INTEGER, TARGET :: NumberOfDOFs, OldIntArray(:), &
          OldIntCnts(:), Reorder(:)
!---------------------------------------------------------------------
     INTEGER :: nstart, nend, doftype
!---------------------------------------------------------------------

     ! Number edges, faces and elements seperately so
     ! that different types of dofs don't get mixed up
     ! (nodes are already numbered)

     ! dead code.
   END SUBROUTINE InitialGlobalNumbering
!---------------------------------------------------------------------

!*********************************************************************
! A parallel numbering algorithm for global DOFs that identifies 
! interface elements by their global node numbers (passed in OldIntCnts and OldIntArray)
!---------------------------------------------------------------------
   SUBROUTINE ParallelNumbering( Mesh, OldIntCnts, OldIntArray, Reorder, & 
        nstart, nend, doftype )
!---------------------------------------------------------------------
    USE GeneralUtils
!---------------------------------------------------------------------
     TYPE(Mesh_t) :: Mesh
     INTEGER, TARGET :: OldIntArray(:), &
          OldIntCnts(:), Reorder(:), nstart, nend, doftype
!---------------------------------------------------------------------
     INTEGER, DIMENSION(MPI_STATUS_SIZE) :: status
     INTEGER :: ierr
     INTEGER :: i,j,k,l,n,MinProc,MaxLcl,MaxGlb,Interfaces,dofs,step
     INTEGER :: LIndex(100), IntN, Gindex, k1, k2
     INTEGER, POINTER :: IntArray(:),IntCnts(:),GIndices(:),Gorder(:)
!---------------------------------------------------------------------
     CALL MPI_BARRIER( MPI_COMM_WORLD, ierr )

     PRINT*, 'PE: ', ParEnv % MyPE, ' Numbering type ', doftype, ' from ', nstart, ' to ', nend
     call flush(6)

     ! if no dofs of this type, return
     IF ( nstart > nend ) THEN
        RETURN
     END IF

! XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
! todo: figure out some value that is usually large enough (the worst case size is quite high)
!       This can get awefully large at times.
     CALL CheckBuffer(10000000)
!
! XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

     PRINT*, 'PE: ', ParEnv % MyPE, ' SIZE(GlobalDOFs) ', SIZE(Mesh % ParallelInfo % GlobalDOFs)
!    PRINT*, 'PE: ', ParEnv % MyPE, ' GlobalDOFs before numbering: ', Mesh % ParallelInfo % GlobalDOFs
     call flush(6)
!
!    find least numbered of the active procs:
!    ----------------------------------------
     DO MinProc = 1,ParEnv % PEs
        IF ( ParEnv % Active( MinProc ) ) EXIT
     END DO

     PRINT*, 'PE: ', ParEnv % MyPE, ' MinProc ', MinProc 
     call flush(6)

!
!    Our maximum global node index:
!    ------------------------------
     MaxLcl = MAXVAL( Mesh % ParallelInfo % GlobalDOFs )
     MaxGlb = MaxLcl

!    Find out current maximum global DOF number...
!    (others send their maxlcl to minproc)
!    --------------------------------------
     IF ( ParEnv % MyPE == MinProc-1 ) THEN
        j = 1
        DO i=MinProc+1,ParEnv % PEs
           IF ( ParEnv % Active(i) ) THEN
              CALL MPI_RECV( k, 1, MPI_INTEGER, i-1, &
                10, MPI_COMM_WORLD, status, ierr )
              MaxGlb = MAX( MaxGlb, k )
           END IF
        END DO
     ELSE
        CALL MPI_BSEND( MaxLcl, 1, MPI_INTEGER, &
          MinProc-1, 10, MPI_COMM_WORLD, ierr )
     END IF

    PRINT*,'PE: ',ParEnv % MyPE,' Initial MaxGlb ',MaxGlb
    call flush(6)

    !
    ! How many dofs per node, edge, face or element
    ! 
    IF (doftype == 1) THEN
       step = 1
    ELSE IF (doftype == 2) THEN
       step = Mesh % MaxEdgeDOFs
    ELSE IF (doftype == 3) THEN
       step = Mesh % MaxFaceDOFs
    ELSE IF (doftype == 4) THEN 
       step = Mesh % MaxElementDOFs
    END IF

!    Recieve global numbers for interface nodes from lower
!    numbered PEs. 
!
!    Interface dofs are identified 
!    by the nodes of an edge, face or element.
!
!    IntCnts = number of nodes
!    IntArray = the nodes
!    ---------------------------------------
    DO i=MinProc,ParEnv % MyPE
       IF ( .NOT. (ParEnv % Active(i) .AND. ParEnv % IsNeighbour(i)) ) CYCLE

       PRINT*,'PE: ',ParEnv % MyPE,' Receiving dof numbers.'
       call flush(6)
       
       ! Get number of interfaces to receive from PE i
       CALL MPI_RECV( Interfaces, 1, MPI_INTEGER, &
            i-1, 14, MPI_COMM_WORLD, status, ierr )
       
       IF ( Interfaces > 0 ) THEN
          ALLOCATE( GIndices(Interfaces), IntCnts(Interfaces) )
          
          CALL MPI_RECV( GIndices, Interfaces, MPI_INTEGER, &
               i-1, 15, MPI_COMM_WORLD, status, ierr )
          
          CALL MPI_RECV( IntCnts, Interfaces, MPI_INTEGER, &
               i-1, 16, MPI_COMM_WORLD, status, ierr )

          CALL MPI_RECV( k, 1, MPI_INTEGER, &
               i-1, 17, MPI_COMM_WORLD, status, ierr )
          
          ALLOCATE( IntArray(k) )

          CALL MPI_RECV( IntArray, SIZE(IntArray), MPI_INTEGER, &
               i-1, 18, MPI_COMM_WORLD, status, ierr )

!          Update our view of the global numbering
!          at the interface nodes, edges, faces and elements.
!          ---------------------------------------
          l = 0
          DO j=1,Interfaces
             Lindex = 0
             IntN = IntCnts(j)
             DO k=1,IntN
                Lindex(k) = SearchNode( Mesh % ParallelInfo, IntArray(l+k), 1, Mesh % NumberOfNodes )
             END DO
             
             ! it is maybe redundant to check that the IntArrays match, but
             ! then again, it might be the only way to identify edge, face and element dofs
             IF ( ALL( Lindex(1:IntN) > 0 ) ) THEN
                !
                ! This set of interface dof belongs to us as well
                ! Now lets find it's position in our GlobalDOFs
                k2 = 0 ! 
                k1 = 0 ! interface number 1..Interfaces 
                DO k=nstart,nend
                   IF ( .NOT. Mesh % ParallelInfo % Interface(k) ) CYCLE

                   k1 = k1 + 1
                   IF ( IntN == OldIntCnts(k1) ) THEN
                      ! If all edges match, then this is our interface dof.
                      IF ( ALL( IntArray(l+1:l+IntN) == &
                           OldIntArray(k2+1:k2+IntN)) ) THEN

                         ! number all DOFs of this edge.
                         DO n=1,step
                            Mesh % ParallelInfo % GlobalDOFs(k+n) = GIndices(j)+n
                         END DO

                         EXIT
                      END IF
                   END IF
                   k2 = k2 + OldIntCnts(k1)
                END DO
             END IF
             l = l + IntN
          END DO
          DEALLOCATE( Gindices, IntCnts, IntArray )
       END IF
    END DO

!
!   Update the current numbering from the 
!   previous PE in line, this will make the 
!   execution strictly serial. Maybe there
!   would be an easier way?
!   (waits here until previous PE has sent Interface nodes and MaxGlb) 
!
    IF ( ParEnv % MyPE > MinProc-1 ) THEN
       DO i=ParEnv % MyPE, MinProc, -1
          IF ( ParEnv % Active(i) ) THEN
             CALL MPI_RECV( MaxGlb, 1, MPI_INTEGER, &
                  i-1, 20, MPI_COMM_WORLD, status, ierr )
             EXIT
          END IF
       END DO
    END IF
!
!    Create global numbering for our dofs that haven't 
!    been numbered yet.
!    ----------------------------------
     DO i=nstart,nend
        IF ( Mesh % ParallelInfo % GlobalDOFs(i) == 0 ) THEN
           MaxGlb = MaxGlb + 1
           Mesh % ParallelInfo % GlobalDOFs(i) = MaxGlb
        END IF
     END DO

     !
     ! count interfaces, with edges, faces etc, we don't need to send all dofs, just the 
     ! first one. others will be successively numbered
     !
     IF ( doftype == 1 ) THEN
        Interfaces = COUNT( Mesh % ParallelInfo % Interface(nstart:nend) )
     ELSE IF ( doftype == 2 ) THEN
        Interfaces = COUNT( Mesh % ParallelInfo % Interface(nstart:nend) ) / Mesh % MaxEdgeDOFs
     ELSE IF ( doftype == 3 ) THEN
        Interfaces = COUNT( Mesh % ParallelInfo % Interface(nstart:nend) ) / Mesh % MaxFaceDOFs
     ELSE IF ( doftype == 4 ) THEN
        Interfaces = COUNT( Mesh % ParallelInfo % Interface(nstart:nend) ) / Mesh % MaxElementDOFs
     END IF

     IF ( Interfaces > 0 ) ALLOCATE( Gindices(Interfaces) )

     Interfaces = 0
     DO i=nstart,nend,step
        IF ( Mesh % ParallelInfo % Interface(i) ) THEN
           Interfaces = Interfaces + 1
           Gindices(Interfaces) = Mesh % ParallelInfo % GlobalDOFs(i)
        END IF
     END DO

     PRINT*,'PE: ',ParEnv % MyPE,' Sending interface dof numbers.'
     call flush(6)

!
!    Send new interface nodes to higher numbered PEs:
!    ------------------------------------------------
     DO i=ParEnv % MyPE+2,ParEnv % PEs
        IF ( .NOT. (ParEnv % Active(i) .AND. ParEnv % IsNeighbour(i)) ) CYCLE

        CALL MPI_BSEND( Interfaces, 1, MPI_INTEGER, &
               i-1, 14, MPI_COMM_WORLD, ierr )

        IF ( Interfaces > 0 ) THEN
           CALL MPI_BSEND( GIndices, Interfaces, MPI_INTEGER, &
                    i-1, 15, MPI_COMM_WORLD, ierr )

           CALL MPI_BSEND( OldIntCnts, Interfaces, &
              MPI_INTEGER, i-1, 16, MPI_COMM_WORLD, ierr )

           CALL MPI_BSEND( SIZE(OldIntArray), 1, &
              MPI_INTEGER, i-1, 17, MPI_COMM_WORLD, ierr )

           CALL MPI_BSEND( OldIntArray, SIZE(OldIntArray), &
              MPI_INTEGER, i-1, 18, MPI_COMM_WORLD, ierr )
        END IF
     END DO

     IF ( Interfaces > 0 ) DEALLOCATE( GIndices )

     PRINT*,'PE: ',ParEnv % MyPE,' Sending MaxGlb ', MaxGlb
     call flush(6)
!
!    Send go singal to next PE in line...
!    ------------------------------------
     DO i = ParEnv % MyPE+2, ParEnv % PEs
        IF ( ParEnv % Active(i) ) THEN
           CALL MPI_BSEND( MaxGlb, 1, MPI_INTEGER, &
               i-1, 20, MPI_COMM_WORLD, ierr )
           EXIT
        END IF
     END DO
!
!    Sort our own nodes according to ascending
!    global order:
!
     ALLOCATE( IntCnts(nend-nstart+1), Gorder(nend-nstart+1) )

     DO i=1,nend-nstart
        Gorder(i) = i
     END DO

     PRINT*,'PE: ',ParEnv % MyPE,' Sorting globaldofs '
     call flush(6)

     CALL SortI( nend-nstart+1, Mesh % ParallelInfo % GlobalDOFs(nstart:nend), Gorder )

!     PRINT*,'PE: ',ParEnv % MyPE,' Gorder ', Gorder

     DO i=1,nend-nstart+1
        IntCnts(Gorder(i)) = i
     END DO

!
!    Reorder will return the nodal reordering
!    to the caller:
!    ----------------------------------------
     Reorder(nstart:nend) = IntCnts + nstart - 1
!
!    Order the whole of the nodal structure
!    according to the changed order of the 
!    global numbers:
!
!    Also take into account the fact that edges, faces 
!    and elements can have more than 1 dof
!    -------------------------------------
      
     PRINT*,'PE: ',ParEnv % MyPE,' Swapping '
     call flush(6)

     DO i=1,nend-nstart,step
        DO l=1,step
           k = Gorder(i)
           j = IntCnts(i)
           IntCnts(i) = IntCnts(k)
           IntCnts(k) = j
           Gorder(IntCnts(k)) = k
        END DO
        CALL Swap( Mesh, i+nstart-1, k+nstart-1, doftype )
     END DO

     DEALLOCATE( IntCnts )


!    Ok, now we have generated the global numbering
!    of nodes. We still need to distribute the
!    information which PEs share which of the new
!    interface nodes:
!    -----------------------------------------------


     Interfaces = COUNT( Mesh % ParallelInfo % Interface(nstart:nend) )

     ! If interface node, allocate neighbourlist
     ALLOCATE( GIndices( Interfaces ) )
     j = 0
     DO i=nstart,nend
        IF ( Mesh % ParallelInfo % Interface(i) ) THEN
           j = j + 1
           GIndices(j) = Mesh % ParallelInfo % GlobalDOFs(i)
           ALLOCATE( Mesh % ParallelInfo % NeighbourList(i) % Neighbours(ParEnv % PEs) )
           Mesh % ParallelInfo % NeighbourList(i) % Neighbours = -1
        END IF
     END DO

     PRINT*,'PE: ',ParEnv % MyPE,' Sending global indices of interface dofs'
     call flush(6)

     ! send interface node count and global indice list to neighbouring partitions
     DO i=MinProc,ParEnv % PEs
        IF ( ParEnv % MyPE == i-1 ) CYCLE
        IF ( ParEnv % Active(i) .AND. ParEnv % IsNeighbour(i) ) THEN

           PRINT*,'PE: ',ParEnv % MyPE,' Sending to ', i-1
           call flush(6)

           CALL MPI_BSEND( Interfaces, 1, &
              MPI_INTEGER, i-1, 30, MPI_COMM_WORLD, ierr )

           CALL MPI_BSEND( GIndices, Interfaces, &
              MPI_INTEGER, i-1, 31, MPI_COMM_WORLD, ierr )
        END IF
     END DO

     DEALLOCATE( Gindices )

     ALLOCATE( IntCnts(nend-nstart+1) )

     ! Set our PE in the neighbourlist
     IntCnts = 0
     DO i=nstart,nend
        IF ( Mesh % ParallelInfo % Interface(i) ) THEN
           IntCnts(i) = IntCnts(i) + 1
           Mesh % ParallelInfo % NeighbourList(i) % Neighbours(1) = ParEnv % MyPE
        END IF
     END DO

     PRINT*,'PE: ',ParEnv % MyPE,' Receiving global indices of interface dofs'
     call flush(6)

     ! receive global indices and the number of interface nodes from other partitions
     DO i=MinProc,ParEnv % PEs
        IF ( ParEnv % MyPE == i-1 ) CYCLE
        IF ( ParEnv % Active(i) .AND. ParEnv % IsNeighbour(i) ) THEN

           PRINT*,'PE: ',ParEnv % MyPE,' Receiving from ', i-1
           call flush(6)

           CALL MPI_RECV( Interfaces, 1, MPI_INTEGER, &
               i-1, 30, MPI_COMM_WORLD, status, ierr )

           ALLOCATE( GIndices(Interfaces) )

           CALL MPI_RECV( GIndices, Interfaces, MPI_INTEGER, &
               i-1, 31, MPI_COMM_WORLD, status, ierr )

           DO j=1,Interfaces
              k = SearchNode( Mesh % ParallelInfo, Gindices(j), nstart, nend )
              ! if we find the global indice in our own globaldofs, then add the sender to neighbourlist
              IF ( k <= 0 ) CYCLE
              IntCnts(k) = IntCnts(k) + 1
              Mesh % ParallelInfo % NeighbourList(k) % Neighbours(IntCnts(k)) = i-1
           END DO

           DEALLOCATE( GIndices )
        END IF
     END DO
!
!    Reallocate the nodal neighbour lists to
!    correct sizes and also sort them (the smallest PE always is owner)
!    ---------------------------------------
     PRINT*,'PE: ',ParEnv % MyPE,' Shrinking neighbourlists'
     call flush(6)

     DO i=nstart,nend
        IF ( Mesh % ParallelInfo % Interface(i) ) THEN
           k = IntCnts(i)
           ALLOCATE( Gindices(k) )
           Gindices = Mesh % ParallelInfo % NeighbourList(i) % Neighbours(1:k)
           CALL Sort( k, Gindices )
           DEALLOCATE( Mesh % ParallelInfo % NeighbourList(i) % Neighbours )
           Mesh % ParallelInfo % NeighbourList(i) % Neighbours => Gindices
        END IF
     END DO
     
     DEALLOCATE( IntCnts )
     
     PRINT*,'PE: ',ParEnv % MyPE,' Done numbering type ', doftype
     call flush(6)

     CALL MPI_BARRIER( MPI_COMM_WORLD, ierr )

CONTAINS
!
! Swap a certain dof typex
!-----------------------------------------------------------------------
  SUBROUTINE Swap( Mesh, i, k, doftype ) 
!-----------------------------------------------------------------------
    INTEGER :: i,k,doftype
    TYPE(Mesh_t) :: Mesh
!-----------------------------------------------------------------------
    INTEGER :: elementi, elementk
    REAL(KIND=dp) :: swapx,swapy,swapz
    TYPE(Element_t) :: Element
!-----------------------------------------------------------------------
        
    ! don't swap if there is no need to swap...
    IF( i /= k ) THEN
!       PRINT*,'PE: ',ParEnv % MyPE, ' Swap: ', i,',',k,' doftype ', doftype
       call flush(6)

       IF ( doftype == 1 ) THEN
          elementi = i
          elementk = k

          swapx =  Mesh % Nodes % x(i)
          swapy =  Mesh % Nodes % y(i)
          swapz =  Mesh % Nodes % z(i)
 
          Mesh % Nodes % x(i) = Mesh % Nodes % x(k)
          Mesh % Nodes % y(i) = Mesh % Nodes % y(k)
          Mesh % Nodes % z(i) = Mesh % Nodes % z(k)
          
          Mesh % Nodes % x(k) = swapx
          Mesh % Nodes % y(k) = swapy
          Mesh % Nodes % z(k) = swapz

          CALL SwapIfaceAndNList( i,k )

       ELSE IF ( doftype == 2 ) THEN

          elementi = (i - Mesh % NumberOfNodes) / Mesh % MaxEdgeDOFs + 1
          elementk = (k - Mesh % NumberOfNodes) / Mesh % MaxEdgeDOFs + 1

          Element = Mesh % Edges(elementi)
          Mesh % Edges(elementk) = Mesh % Edges(elementi)
          Mesh % Edges(elementi) = Element

          DO l=1,Mesh % MaxEdgeDOFs
             CALL SwapIfaceAndNList( i + l - 1, k + l - 1 )
          END DO

       ELSE IF ( doftype == 3 ) THEN

          elementi = (i - Mesh % NumberOfNodes - &
               Mesh % NumberOfEdges * Mesh % MaxEdgeDOFs) / Mesh % MaxFaceDOFs + 1
          elementi = (k - Mesh % NumberOfNodes - & 
               Mesh % NumberOfEdges * Mesh % MaxEdgeDOFs) / Mesh % MaxFaceDOFs + 1

          Element = Mesh % Faces(elementi)
          Mesh % Faces(elementk) = Mesh % Faces( elementi )
          Mesh % Faces(elementi) = Element

          DO l=1,Mesh % MaxFaceDOFs
             CALL SwapIfaceAndNList( i + l - 1, k + l - 1 )
          END DO

       ELSE IF ( doftype == 4 ) THEN

          elementi = (i - Mesh % NumberOfNodes - & 
               Mesh % NumberOfEdges * Mesh % MaxEdgeDOFs - &
               Mesh % NumberOfFaces * Mesh % MaxFaceDOFs) / Mesh % MaxElementDOFs + 1
          elementk = (k - Mesh % NumberOfNodes - & 
               Mesh % NumberOfEdges * Mesh % MaxEdgeDOFs - &
               Mesh % NumberOfFaces * Mesh % MaxFaceDOFs) / Mesh % MaxElementDOFs + 1

          Element = Mesh % Elements(elementi)
          Mesh % Elements(elementk) = Mesh % Elements(elementi)
          Mesh % Elements(elementi) = Element

          DO l=1,Mesh % MaxElementDOFs
             CALL SwapIfaceAndNList( i + l - 1, k + l - 1 )
          END DO

       END IF

    END IF
!-----------------------------------------------------------------------
  END SUBROUTINE Swap

  ! Swap interface table and neighbourlists between dofs 
  !
  SUBROUTINE SwapIfaceAndNList( i,k )
!-----------------------------------------------------------------------
    INTEGER :: i,k
!-----------------------------------------------------------------------
    LOGICAL :: swapi
    INTEGER, POINTER :: swapl(:)
!-----------------------------------------------------------------------
    swapi =  Mesh % ParallelInfo % Interface(i)
    swapl => Mesh % ParallelInfo % NeighbourList(i) % Neighbours
    
    Mesh % ParallelInfo % Interface(i) = Mesh % ParallelInfo % Interface(k) 
    Mesh % ParallelInfo % NeighbourList(i) % Neighbours => &
         Mesh % ParallelInfo % NeighbourList(k) % Neighbours
    
    Mesh % ParallelInfo % Interface(k) = swapi
    Mesh % ParallelInfo % NeighbourList(k) % Neighbours => swapl
  END SUBROUTINE SwapIfaceAndNList

!-----------------------------------------------------------------------
END SUBROUTINE ParallelNumbering
!-----------------------------------------------------------------------


!-----------------------------------------------------------------------
SUBROUTINE SParIterBarrier
!-----------------------------------------------------------------------
  INTEGER :: ierr
  CALL MPI_BARRIER( MPI_COMM_WORLD, ierr )
!-----------------------------------------------------------------------
END  SUBROUTINE SParIterBarrier
!-----------------------------------------------------------------------


!-----------------------------------------------------------------------
SUBROUTINE SParIterActiveBarrier
!-----------------------------------------------------------------------
  INTEGER :: i,j,k=0,ierr, status(MPI_STATUS_SIZE)

  DO i=1,ParEnv % PEs
    IF ( ParEnv % Active(i) ) EXIT
  END DO 

  IF ( ParEnv % myPE == i-1 ) THEN
    DO j=1,COUNT(ParEnv % Active)-1
      CALL MPI_RECV( k, 1, MPI_INTEGER, &
       MPI_ANY_SOURCE, 701, MPI_COMM_WORLD, status, ierr )
    END DO
    DO j=1,ParEnv % PEs
      IF ( ParEnv % Active(j) .AND. j-1 /= ParEnv % myPE ) THEN
        CALL MPI_BSEND( k, 1, MPI_INTEGER, &
             j-1, 702, MPI_COMM_WORLD, status, ierr )
      END IF
    END DO
  ELSE
    CALL MPI_BSEND( k, 1, MPI_INTEGER, &
      i-1, 701, MPI_COMM_WORLD, status, ierr )

    CALL MPI_RECV( k, 1, MPI_INTEGER, &
      i-1, 702, MPI_COMM_WORLD, status, ierr )
  END IF
!-----------------------------------------------------------------------
END  SUBROUTINE SParIterActiveBarrier
!-----------------------------------------------------------------------


!-----------------------------------------------------------------------
SUBROUTINE SParIterAllReduceAnd(L)
   LOGICAL :: L
!-----------------------------------------------------------------------
   INTEGER :: ierr
   CALL MPI_ALLREDUCE(L,L,1,MPI_LOGICAL,MPI_LAND,MPI_COMM_WORLD,ierr)
!-----------------------------------------------------------------------
END  SUBROUTINE SParIterAllReduceAnd
!-----------------------------------------------------------------------


!*********************************************************************
!*********************************************************************
!
! Send all of the interface matrix blocks (in NbsIfMatrices) to neighbour
! processors. This is done only once so there is no need to optimize
! communication...
!
  SUBROUTINE ExchangeInterfaces( NbsIfMatrix, RecvdIfMatrix )

    USE types
    IMPLICIT NONE

    ! Parameters

    TYPE (Matrix_t), DIMENSION(*) :: NbsIfMatrix, RecvdIfMatrix

    ! Local variables

    INTEGER :: i, j, ierr, sproc, destproc, rows, cols, TotalSize

    INTEGER, DIMENSION(MPI_STATUS_SIZE) :: status
  !*********************************************************************

  TotalSize = 0
  DO i = 1, ParEnv % PEs
     IF ( ParEnv % IsNeighbour(i) ) THEN
       IF ( NbsIfMatrix(i) % NumberOfRows == 0 ) THEN
           TotalSize = TotalSize + 4
       ELSE
           Cols = NbsIfMatrix(i) % Rows(NbsIfMatrix(i) % NumberOfRows+1)-1
           TotalSize = TotalSize + 4 + 8*NbsIfMatrix(i) % NumberOfRows + &
                12*Cols
       END IF
     END IF
  END DO
  TotalSize = 1.5 * TotalSize
  CALL CheckBuffer( TotalSize )

  !----------------------------------------------------------------------
  !
  ! Send the interface parts
  !
  !----------------------------------------------------------------------

    DO i = 1, ParEnv % PEs
       IF ( ParEnv % IsNeighbour(i) ) THEN

          destproc = i - 1
          IF ( NbsIfMatrix(i) % NumberOfRows == 0 ) THEN
             CALL MPI_BSEND( 0, 1, MPI_INTEGER, destproc, &
                    1000, MPI_COMM_WORLD, ierr )
          ELSE
             Cols = NbsIfMatrix(i) % Rows(NbsIfMatrix(i) % NumberOfRows+1) - 1

             CALL MPI_BSEND( NbsIfMatrix(i) % NumberOfRows, 1, MPI_INTEGER, &
                  destproc, 1000, MPI_COMM_WORLD, ierr )

             CALL MPI_BSEND( Cols, 1, MPI_INTEGER, &
                  destproc, 1001, MPI_COMM_WORLD, ierr )

             CALL MPI_BSEND( NbsIfMatrix(i) % GRows, &
                  NbsIfMatrix(i) % NumberOfRows, MPI_INTEGER, &
                  destproc, 1002, MPI_COMM_WORLD, ierr )

             CALL MPI_BSEND( NbsIfMatrix(i) % Rows, &
                  NbsIfMatrix(i) % NumberOfRows + 1, MPI_INTEGER, &
                  destproc, 1003, MPI_COMM_WORLD, ierr )

             CALL MPI_BSEND( NbsIfMatrix(i) % RowOwner, &
                  NbsIfMatrix(i) % NumberOfRows, MPI_INTEGER, &
                  destproc, 1004, MPI_COMM_WORLD, ierr )

             CALL MPI_BSEND( NbsIfMatrix(i) % Cols, &
                  Cols, MPI_INTEGER, destproc, 1005, MPI_COMM_WORLD, ierr )
        END IF
     END IF
  END DO

  !----------------------------------------------------------------------
  !
  ! Receive the interface parts
  !
  !----------------------------------------------------------------------

  DO i = 1, ParEnv % NumOfNeighbours
     CALL MPI_RECV( rows, 1, MPI_INTEGER, MPI_ANY_SOURCE, 1000, &
                    MPI_COMM_WORLD, status, ierr )
     sproc = status(MPI_SOURCE)

     IF ( Rows == 0 ) THEN
        RecvdIfMatrix(sproc+1) % NumberOfRows = 0
     ELSE
       CALL MPI_RECV( cols, 1, MPI_INTEGER, sproc, 1001, &
              MPI_COMM_WORLD, status, ierr )

       RecvdIfMatrix(sproc+1) % NumberOfRows = Rows
       ALLOCATE( RecvdIfMatrix(sproc+1) % Rows(Rows+1) )
       ALLOCATE( RecvdIfMatrix(sproc+1) % Diag(Rows) )
       ALLOCATE( RecvdIfMatrix(sproc+1) % Cols(Cols) )
       ALLOCATE( RecvdIfMatrix(sproc+1) % GRows(Rows) )
       ALLOCATE( RecvdIfMatrix(sproc+1) % RowOwner(Rows) )

       CALL MPI_RECV( RecvdIfMatrix(sproc+1) % GRows, Rows,  MPI_INTEGER, &
             sproc, 1002, MPI_COMM_WORLD, status, ierr )

       CALL MPI_RECV( RecvdIfmatrix(sproc+1) % Rows, Rows+1, MPI_INTEGER, &
            sproc, 1003, MPI_COMM_WORLD, status, ierr )

       CALL MPI_RECV( RecvdIfmatrix(sproc+1) % RowOwner, Rows, MPI_INTEGER, &
            sproc, 1004, MPI_COMM_WORLD, status, ierr )

       CALL MPI_RECV( RecvdIfMatrix(sproc+1) % Cols, Cols, MPI_INTEGER, &
            sproc, 1005, MPI_COMM_WORLD, status, ierr )
     END IF
  END DO
!*********************************************************************
END SUBROUTINE ExchangeInterfaces
!*********************************************************************


!*********************************************************************
!*********************************************************************
!
! Send all of the *VALUES* in the interface matrix blocks (in NbsIfMatrices)
! to neighbour processors. This is done once on every non-linear iteration
! when the coefficient matrix has been assembled.
!
SUBROUTINE ExchangeIfValues( NbsIfMatrix, RecvdIfMatrix, NeedMass, NeedDamp )

  USE types
  IMPLICIT NONE

  ! Parameters

  TYPE (Matrix_t), DIMENSION(:) :: NbsIfMatrix, RecvdIfMatrix

  ! Local variables

  LOGICAL :: NeedMass, NeedDamp
  INTEGER, DIMENSION(MPI_STATUS_SIZE) :: status
  INTEGER :: i, j, ierr, destproc, rows, cols, sproc, TotalSize

  !*********************************************************************

  !----------------------------------------------------------------------
  !
  ! Send the interface parts
  !
  !----------------------------------------------------------------------

  TotalSize = 0
  DO i = 1, ParEnv % PEs
     IF ( ParEnv % IsNeighbour(i) ) THEN
        IF ( NbsIfMatrix(i) % NumberOfRows == 0 ) THEN
           TotalSize = TotalSize + 4
        ELSE
           Cols = NbsIfMatrix(i) % Rows(NbsIfMatrix(i) % NumberOfRows+1)-1
           TotalSize = TotalSize + 4 + 8*NbsIfMatrix(i) % NumberOfRows + &
                       12*Cols
        END IF
     END IF
  END DO
  TotalSize = 1.5 * TotalSize
  CALL CheckBuffer( TotalSize )

  DO i = 1, ParEnv % PEs
     IF ( ParEnv % IsNeighbour(i) ) THEN
        destproc = i - 1
        IF ( NbsIfMatrix(i) % NumberOfRows == 0 ) THEN
          CALL MPI_BSEND( 0, 1, MPI_INTEGER, &
            destproc, 2000, MPI_COMM_WORLD, ierr )
        ELSE
          Cols = NbsIfMatrix(i) % Rows(NbsIfMatrix(i) % NumberOfRows+1) - 1

          CALL MPI_BSEND( NbsIfMatrix(i) % NumberOfRows, 1, MPI_INTEGER, &
                destproc, 2000, MPI_COMM_WORLD, ierr )

          CALL MPI_BSEND( Cols, 1, MPI_INTEGER, &
                 destproc, 2001, MPI_COMM_WORLD, ierr )

          CALL MPI_BSEND( NbsIfMatrix(i) % GRows, &
              NbsIfMatrix(i) % NumberOfRows, MPI_INTEGER, &
                destproc, 2002, MPI_COMM_WORLD, ierr )

          CALL MPI_BSEND( NbsIfMatrix(i) % Rows, &
               NbsIfMatrix(i) % NumberOfRows + 1, MPI_INTEGER, &
               destproc, 2003, MPI_COMM_WORLD, ierr )

          CALL MPI_BSEND( NbsIfMatrix(i) % Cols, &
            Cols, MPI_INTEGER, destproc, 2004, MPI_COMM_WORLD, ierr )

          CALL MPI_BSEND( NbsIfMatrix(i) % Values, Cols, &
             MPI_DOUBLE_PRECISION, destproc, 2005, MPI_COMM_WORLD, ierr )

           IF ( NeedMass ) &
              CALL MPI_BSEND( NbsIfMatrix(i) % MassValues, Cols, &
                MPI_DOUBLE_PRECISION, destproc, 2006, MPI_COMM_WORLD, ierr )

           IF ( NeedDamp ) &
              CALL MPI_BSEND( NbsIfMatrix(i) % DampValues, Cols, &
                MPI_DOUBLE_PRECISION, destproc, 2006, MPI_COMM_WORLD, ierr )
           END IF
     END IF
  END DO

  !----------------------------------------------------------------------
  !
  ! Receive the interface parts
  !
  !----------------------------------------------------------------------

  DO i = 1, ParEnv % NumOfNeighbours
     CALL MPI_RECV( Rows, 1, MPI_INTEGER, MPI_ANY_SOURCE, &
             2000, MPI_COMM_WORLD, status, ierr )
     sproc = status(MPI_SOURCE)

     IF ( Rows == 0 ) THEN
        RecvdIfMatrix(sproc+1) % NumberOfRows = 0
     ELSE
        CALL MPI_RECV( Cols, 1, MPI_INTEGER, sproc, 2001, &
               MPI_COMM_WORLD, status, ierr )

        RecvdIfMatrix(sproc+1) % NumberOfRows = Rows
        ALLOCATE( RecvdIfMatrix(sproc+1) % Rows(Rows+1) )
        ALLOCATE( RecvdIfMatrix(sproc+1) % Cols(Cols) )
        ALLOCATE( RecvdIfMatrix(sproc+1) % GRows(Rows) )
        ALLOCATE( RecvdIfMatrix(sproc+1) % Values(Cols) )
        RecvdIfMatrix(sproc+1) % MassValues => NULL()
        IF ( NeedMass ) THEN
          ALLOCATE( RecvdIfMatrix(sproc+1) % MassValues(Cols) )
          RecvdIfMatrix(sproc+1) % MassValues = 0.0d0
        END IF
        RecvdIfMatrix(sproc+1) % DampValues => NULL()
        IF ( NeedDamp ) THEN
          ALLOCATE( RecvdIfMatrix(sproc+1) % DampValues(Cols) )
          RecvdIfMatrix(sproc+1) % DampValues = 0.0d0
        END IF

        CALL MPI_RECV( RecvdIfMatrix(sproc+1) % GRows, Rows,  MPI_INTEGER, &
                  sproc, 2002, MPI_COMM_WORLD, status, ierr )

        CALL MPI_RECV( RecvdIfmatrix(sproc+1) % Rows, Rows+1, MPI_INTEGER, &
                  sproc, 2003, MPI_COMM_WORLD, status, ierr )

        CALL MPI_RECV( RecvdIfMatrix(sproc+1) % Cols, Cols, MPI_INTEGER, &
                  sproc, 2004, MPI_COMM_WORLD, status, ierr )

        CALL MPI_RECV( RecvdIfMatrix(sproc+1) % Values, Cols, &
            MPI_DOUBLE_PRECISION, sproc, 2005, MPI_COMM_WORLD, status, ierr )

        IF ( NeedMass ) &
           CALL MPI_RECV( RecvdIfMatrix(sproc+1) % MassValues, Cols, &
             MPI_DOUBLE_PRECISION, sproc, 2006, MPI_COMM_WORLD, status, ierr )

        IF ( NeedDamp ) &
           CALL MPI_RECV( RecvdIfMatrix(sproc+1) % DampValues, Cols, &
             MPI_DOUBLE_PRECISION, sproc, 2006, MPI_COMM_WORLD, status, ierr )
     END IF

  END DO
!*********************************************************************
END SUBROUTINE ExchangeIfValues
!*********************************************************************


!*********************************************************************
!*********************************************************************
!
! Exchange right-hand-side elements on the interface with neighbours
!
SUBROUTINE ExchangeRHSIf( SourceMatrix, SplittedMatrix, &
          ParallelInfo, SourceRHS, TargetRHS )

  TYPE (SplittedMatrixT) :: SplittedMatrix
  TYPE (Matrix_t) :: SourceMatrix
  TYPE (ParallelInfo_t) :: ParallelInfo
  REAL(KIND=dp), DIMENSION(:) :: SourceRHS, TargetRHS

  ! Local variables

  INTEGER :: i, j, k, datalen, ierr, sproc, destproc, ind, psum, dl, DOF
  INTEGER :: owner, request
  INTEGER, DIMENSION(MPI_STATUS_SIZE) :: status
  INTEGER, DIMENSION(:), ALLOCATABLE :: indbuf
  REAL(KIND=dp), DIMENSION(:), ALLOCATABLE :: valbuf
  INTEGER, DIMENSION(:), ALLOCATABLE :: RHSbufInds

  !*********************************************************************

  !----------------------------------------------------------------------
  !
  ! Extract the interface elements from SourceRHS to be sent to the
  ! real owner of that element.
  !
  !----------------------------------------------------------------------

  ALLOCATE(RHSbufInds(ParEnv % PEs))
  RHSbufInds = 0
  DO i = 1, SourceMatrix % NumberOfRows
    k = SourceMatrix % INVPerm(i)
    owner = ParallelInfo % NeighbourList(k) % Neighbours(1)
    IF ( owner /= ParEnv % MyPE .AND. ParEnv % Active(owner+1) ) THEN
       RHSbufInds(owner+1) = RHSbufInds(owner+1) + 1
       SplittedMatrix % RHS(owner+1) % RHSind(RHSbufInds(owner+1)) = &
             ParallelInfo % GlobalDOFs(k)
       SplittedMatrix % RHS(owner+1) % RHSvec(RHSbufInds(owner+1)) = &
            SourceRHS(i)
    END IF
  END DO

  !----------------------------------------------------------------------
  !
  ! Do the exchange operation
  !
  !----------------------------------------------------------------------

  DO i = 1, ParEnv % PEs
     IF ( ParEnv % IsNeighbour(i) ) THEN
        destproc = i - 1
        DataLen = RHSbufInds(i)

        IF ( DataLen <= 0 ) THEN
           CALL MPI_BSEND( 0, 1, MPI_INTEGER, &
                destproc, 3000, MPI_COMM_WORLD, ierr )
        ELSE
           CALL MPI_BSEND( DataLen, 1, MPI_INTEGER, &
                destproc, 3000, MPI_COMM_WORLD, ierr )

           CALL MPI_BSEND( SplittedMatrix % RHS(i) % RHSind, DataLen, &
              MPI_INTEGER, destproc, 3001, MPI_COMM_WORLD, ierr )

           CALL MPI_BSEND( SplittedMatrix % RHS(i) % RHSVec, DataLen, &
              MPI_DOUBLE_PRECISION, destproc, 3002, MPI_COMM_WORLD, ierr )
        END IF

     END IF
  END DO

  DO i = 1, ParEnv % NumOfNeighbours
     CALL MPI_RECV( DataLen, 1, MPI_INTEGER, MPI_ANY_SOURCE, &
               3000, MPI_COMM_WORLD, status, ierr )
     sproc = status(MPI_SOURCE)

     IF ( DataLen /= 0 ) THEN
        ALLOCATE( IndBuf(DataLen), ValBuf(DataLen) )

        CALL MPI_RECV( IndBuf, DataLen, MPI_INTEGER, sproc, &
                3001, MPI_COMM_WORLD, status, ierr )
        CALL MPI_RECV( ValBuf, DataLen, MPI_DOUBLE_PRECISION, &
             sproc, 3002, MPI_COMM_WORLD, status, ierr )
        DO j = 1, DataLen
           Ind = SearchNode( ParallelInfo, IndBuf(j) )
           IF ( Ind /= -1 ) THEN
              Ind = SourceMatrix % Perm(Ind)
              IF ( Ind > 0 ) THEN
                 SourceRHS(Ind) = SourceRHS(Ind) + ValBuf(j)
              END IF
           ELSE
!             WRITE( Message, * ) ParEnv % MyPE,'RHS receive error'
!             CALL Fatal( 'ExchangeRHSIf', Message )
           END IF
        END DO
        DEALLOCATE( IndBuf, ValBuf )
     END IF
     
  END DO

  ! Clean up temporary work space
  DEALLOCATE( RHSbufInds )

  ! copy source to target
  j = 0
  DO i = 1, SourceMatrix % NumberOfRows
     k = SourceMatrix % INVPerm(i)
     IF (ParallelInfo % NeighbourList(k) % Neighbours(1)==ParEnv % MyPE) THEN
        j = j + 1
        TargetRHS(j) = SourceRHS(i)
     END IF
  END DO
!*********************************************************************
END SUBROUTINE ExchangeRHSIf
!*********************************************************************


!*********************************************************************
!*********************************************************************
!
! Build index tables for faster vector element combination (in parallel
! matrix-vector operation).
!
SUBROUTINE BuildRevVecIndices( SplittedMatrix )
  USE Types
  IMPLICIT NONE

  TYPE (SplittedMatrixT) :: SplittedMatrix
  TYPE (Nodes_t) :: Nodes

  ! Local variables

  TYPE (Matrix_t), POINTER :: CurrIf, InsideMatrix
  INTEGER :: i, j, k, l, n, VecLen, ind, ierr, destproc, sproc, TotLen
  INTEGER :: TotalSize, FoundCount,ncount
  INTEGER, POINTER :: RevBuff(:),RevInd(:)
  INTEGER, DIMENSION(MPI_STATUS_SIZE) :: status
  INTEGER, DIMENSION(:), ALLOCATABLE :: GIndices,RowOwner,nproc
  LOGICAL :: Found
  LOGICAL*1, ALLOCATABLE :: Done(:,:)
REAL(KIND=dp) :: tt,CPUTime

  !*********************************************************************
tt = CPUTime()

  TotalSize = 0
  DO i = 1, ParEnv % PEs
     IF ( ParEnv % IsNeighbour(i) ) THEN
        CurrIf => SplittedMatrix % NbsIfMatrix(i)
        IF ( CurrIf % NumberOfRows == 0 ) THEN
           TotalSize = TotalSize + 4
        ELSE
           k = CurrIf % Rows(CurrIf % NumberOfRows+1)-1
           TotalSize = TotalSize + 4 + 8*CurrIf % NumberOfRows + 12*k
        END IF
     END IF

     IF ( ParEnv % IsNeighbour(i) ) THEN
        CurrIf => SplittedMatrix % IfMatrix(i)
        IF ( CurrIf % NumberOfRows == 0 ) THEN
           TotalSize = TotalSize + 4
        ELSE
           k = CurrIf % Rows(CurrIf % NumberOfRows+1)-1
           TotalSize = TotalSize + 4 + 8*CurrIf % NumberOfRows + 12*k
        END IF
     END IF
  END DO

  TotalSize = 1.5 * TotalSize
  CALL CheckBuffer( TotalSize )

  InsideMatrix => SplittedMatrix % InsideMatrix

  ncount = 0
  ALLOCATE( nproc(Parenv % PEs) )
  DO i=1,parenv % Pes
     IF ( ParEnv % IsNeighbour(i) ) THEN
        ncount = ncount + 1
        nproc(i) = ncount
     END IF
  END DO
  ALLOCATE( Done(ncount, InsideMatrix % NumberOfRows) )
  Done = .FALSE.

  DO i = 1, ParEnv % PEs
     CurrIf => SplittedMatrix % IfMatrix(i)
     ALLOCATE( GIndices( CurrIf % NumberOfRows ) )

     DO j=0,ParEnv % PEs-1
        IF (  ParEnv % IsNeighbour(j+1) ) THEN
           L = 0
           DO k=1,CurrIf % NumberOfRows
              IF ( CurrIf % RowOwner(k) == j ) THEN
                 L = L + 1
                 GIndices(L) = CurrIf % GRows(k)
              END IF
           END DO
           CALL MPI_BSEND( L, 1, MPI_INTEGER, j, 4000 + 2*i, &
                    MPI_COMM_WORLD, ierr )

           IF ( L > 0 ) THEN
              CALL MPI_BSEND( GIndices, L, MPI_INTEGER, j, &
                 4000 + 2*i+1, MPI_COMM_WORLD, ierr )
           END IF
        END IF
     END DO
     DEALLOCATE( GIndices )
  END DO
!print*,parenv % mype, 'first send: ', CPUTime()-tt
!tt = CPUtime()
!
!
!
  DO i = 1, ParEnv % PEs
     IF ( .NOT. ParEnv % IsNeighbour(i) ) CYCLE

     CurrIf => SplittedMatrix % IfMatrix(i)

     ALLOCATE( RevBuff(1000) )
     RevInd => RevBuff
     RevInd = 0
     TotLen = 0
     sproc = i-1

     DO j=0,ParEnv % PEs-1
        CALL MPI_RECV( VecLen, 1, MPI_INTEGER, sproc, &
          4000+2*(j+1), MPI_COMM_WORLD, status, ierr )

        IF ( VecLen /= 0 ) THEN
           ALLOCATE( GIndices(VecLen) )

           CALL MPI_RECV( GIndices, VecLen, MPI_INTEGER, sproc, &
              4000+2*(j+1)+1, MPI_COMM_WORLD, status, ierr )

           IF ( TotLen + VecLen > SIZE(RevBuff) ) THEN
              ALLOCATE( RevInd( 2*(TotLen + VecLen) ) )
              RevInd = 0
              RevInd(1:SIZE(RevBuff)) = RevBuff
              DEALLOCATE( RevBuff )
              RevBuff => RevInd 
              Revind => RevBuff(TotLen+1:)
           END IF

           FoundCount = 0
           DO n = 1, VecLen
              Ind = SearchIAItem( InsideMatrix %  NumberOfRows, &
                InsideMatrix % GRows, GIndices(n), InsideMatrix % Gorder )

              IF ( Ind > 0 ) THEN
                 RevInd(n) = Ind

                 IF ( Done(nproc(i),Ind) ) CYCLE
                 Done(nproc(i),Ind) = .TRUE.

                 Found = .FALSE.
                 DO k = 1,CurrIf % NumberOfRows
                    DO l = CurrIf % Rows(k), CurrIf % Rows(k+1) - 1
                       IF ( Currif % Cols(l) == GIndices(n) ) THEN
                          SplittedMatrix % IfLCols(i) % IfVec(l) = ind
                          Found = .TRUE.
                          EXIT
                       END IF
                    END DO
                 END DO
                 FoundCount = FoundCount + 1
              ELSE
!                WRITE( Message, * ) ParEnv % MyPE,' (1)Could not find local node ', &
!                         GIndices(n), '(reveiced from', sproc, ')'
!                CALL Error( 'BuildRevVecIndices', Message )
              END IF
           END DO

           DEALLOCATE( GIndices )
           Totlen = TotLen + VecLen
           RevInd => RevBuff(TotLen+1:)
        END IF 
     END DO

     IF ( TotLen > 0 ) THEN
       SplittedMatrix % VecIndices(i) % RevInd => RevBuff
     ELSE
       DEALLOCATE( RevBuff )
       NULLIFY( SplittedMatrix % VecIndices(i) % RevInd )
     END IF
  END DO

!print*,parenv % mype, 'first recv: ', CPUTime()-tt
!tt = CPUtime()

  DO i = 1, ParEnv % PEs
     IF ( .NOT. ParEnv % IsNeighbour(i) ) CYCLE

     CurrIf => SplittedMatrix % NbsIfMatrix(i)
     IF ( CurrIf % NumberOfRows <= 0 ) THEN
        CALL MPI_BSEND( 0, 1, MPI_INTEGER, i-1, 5000, MPI_COMM_WORLD, ierr )
     ELSE
        VecLen = CurrIf % Rows(CurrIf % NumberOfRows+1) - 1
        CALL MPI_BSEND(VecLen, 1, MPI_INTEGER, i-1, &
                5000, MPI_COMM_WORLD, ierr)

        CALL MPI_BSEND( CurrIf % Cols, VecLen, MPI_INTEGER, &
               i-1, 5001, MPI_COMM_WORLD, ierr )
     END IF
  END DO

!print*,parenv % mype, 'secnd send: ', CPUTime()-tt
!tt = CPUtime()

  DO i = 1, ParEnv % PEs
     IF ( .NOT. ParEnv % IsNeighbour(i)) CYCLE

     CurrIf => SplittedMatrix % IfMatrix(i)
     sproc = i-1

     CALL MPI_RECV( VecLen, 1, MPI_INTEGER, sproc, 5000, &
               MPI_COMM_WORLD, status, ierr )

     IF ( VecLen /= 0 ) THEN
        ALLOCATE( GIndices(VecLen) )

        CALL MPI_RECV( GIndices, VecLen, MPI_INTEGER, sproc, &
               5001, MPI_COMM_WORLD, status, ierr )

        DO n = 1, VecLen
           Ind = SearchIAItem( InsideMatrix % NumberOfRows, &
               InsideMatrix % GRows, GIndices(n), InsideMatrix % Gorder )

           IF ( Ind > 0 ) THEN
              IF ( Done(nproc(i),Ind ) ) CYCLE
              Done(nproc(i),Ind) = .TRUE.

              DO k = 1,CurrIf % NumberOfRows
                 DO l = CurrIf % Rows(k), CurrIf % Rows(k+1) - 1
                    IF ( Currif % Cols(l) == GIndices(n) ) THEN
                       SplittedMatrix % IfLCols(i) % IfVec(l) = Ind
                        EXIT
                    END IF
                 END DO
              END DO
                 
           ELSE
!             WRITE( Message, * ) ParEnv % MyPE,'(2) Could not find local node ', &
!                      GIndices(n), '(reveiced from', sproc, ')'
!             CALL Error( 'BuildRevVecIndices', Message )
           END IF
        END DO

        DEALLOCATE( GIndices )
     END IF
  END DO
  DEALLOCATE( Done, nproc )
!print*,parenv % mype, 'secnd recv: ', CPUTime()-tt
!*********************************************************************
END SUBROUTINE BuildRevVecIndices
!*********************************************************************


!*********************************************************************
!*********************************************************************
!
! Send our part of the interface matrix blocks to neighbours.
!
SUBROUTINE Send_LocIf( SplittedMatrix )

  USE types
  IMPLICIT NONE

  TYPE (SplittedMatrixT) :: SplittedMatrix

  ! Local variables

  INTEGER :: i, j, k, ierr, TotalL
  TYPE (Matrix_t), POINTER :: IfM
  TYPE (IfVecT), POINTER :: IfV
  INTEGER, ALLOCATABLE :: L(:)
  REAL(KIND=dp), ALLOCATABLE :: VecL(:,:)

  !*********************************************************************

  ALLOCATE( L(ParEnv % PEs) )
  L = 0
  TotalL = 0

  DO i = 1, ParEnv % PEs
     IfM => SplittedMatrix % IfMatrix(i)

     DO j=1,ParEnv % PEs
        IF ( .NOT. ParEnv % IsNeighbour(j) ) CYCLE

        DO k=1,IfM % NumberOfRows
           IF ( IfM % RowOwner(k) == j-1 ) THEN
              L(j) = L(j) + 1
              TotalL = TotalL + 1
           END IF
        END DO
     END DO
  END DO

  ALLOCATE( VecL( MAXVAL(L), ParEnv % PEs ) )
  L = 0
  VecL = 0

  CALL CheckBuffer( 12*TotalL )

  DO i = 1, ParEnv % PEs
     IfM => SplittedMatrix % IfMatrix(i)
     IfV => SplittedMatrix % IfVecs(i)

     DO j=1, ParEnv % PEs
        IF ( .NOT. ParEnv % IsNeighbour(j) ) CYCLE

        DO k=1,IfM % NumberOfRows
           IF ( IfM % RowOwner(k) == j-1 ) THEN
              L(j) = L(j) + 1
              VecL(L(j),j) = IfV % IfVec(k)
           END IF
        END DO
     END DO
  END DO

  DO j=1,ParEnv % PEs
     IF ( .NOT. ParEnv % IsNeighbour(j) ) CYCLE

     CALL MPI_BSEND( L(j), 1, MPI_INTEGER, J-1, 6000, &
                MPI_COMM_WORLD, IERR )

     IF ( L(j) > 0 ) THEN
        CALL MPI_BSEND( VecL(1:L(j),j), L(j), MPI_DOUBLE_PRECISION, &
                 J-1, 6001, MPI_COMM_WORLD, ierr )
     END IF
  END DO

  IF ( ALLOCATED(VecL) ) DEALLOCATE( VecL, L )

!*********************************************************************
END SUBROUTINE Send_LocIf
!*********************************************************************


!*********************************************************************
!*********************************************************************
!
! Receive interface block contributions to vector from neighbours
!
SUBROUTINE Recv_LocIf( SplittedMatrix, ndim, v )

  USE types
  IMPLICIT NONE

  TYPE (SplittedMatrixT) :: SplittedMatrix
  INTEGER :: ndim
  REAL(KIND=dp), DIMENSION(*) :: v
  REAL(KIND=dp), ALLOCATABLE :: DPBuffer(:)

  SAVE DPBuffer

  ! Local variables

  integer :: i, j, k, ierr, sproc
  integer, dimension(MPI_STATUS_SIZE) :: status

  INTEGER, POINTER :: RevInd(:)
  INTEGER :: VecLen, TotLen

  !*********************************************************************

  IF ( .NOT. ALLOCATED(DPBuffer) ) ALLOCATE(DPBuffer(ndim)) 

  DO i = 1, ParEnv % NumOfNeighbours
     CALL MPI_RECV( VecLen, 1, MPI_INTEGER, MPI_ANY_SOURCE, &
              6000, MPI_COMM_WORLD, status, ierr )

     IF ( VecLen > 0 ) THEN
        sproc = status(MPI_SOURCE)
        RevInd => SplittedMatrix % VecIndices(sproc+1) % RevInd

        IF ( VecLen > SIZE( DPBuffer ) ) THEN
           DEALLOCATE( DPBuffer )
           ALLOCATE( DPBuffer( VecLen ) )
        END IF

        CALL MPI_RECV( DPBuffer, VecLen, MPI_DOUBLE_PRECISION, &
               sproc, 6001, MPI_COMM_WORLD, status, ierr )

        DO k = 1, VecLen
           IF ( RevInd(k) > 0 ) THEN
              v(RevInd(k)) = v(RevInd(k)) + DPBuffer(k)
           ELSE
!             WRITE( Message, * ) ParEnv % MyPE, 'If Receive error: ', k,RevInd(k)
!             CALL Error( 'Recv_LocIf', Message )
           END IF
        END DO
     END IF
  END DO
!*********************************************************************
END SUBROUTINE Recv_LocIf
!*********************************************************************


!*********************************************************************
SUBROUTINE SParActiveSUM(tsum)
   REAL(KIND=dp) :: tsum
!*********************************************************************
   REAL(KIND=dp) :: ssum,s,RealTime
   INTEGER :: i,n,twop,nActive,myid,ierr
   INTEGER :: status(MPI_STATUS_SIZE)
   INTEGER, ALLOCATABLE :: ActiveId(:)
   SAVE ActiveId

!s = RealTime()
   IF ( COUNT(ParEnv % Active)==ParEnv % PEs ) THEN
     ssum = tsum
     CALL MPI_ALLREDUCE( ssum, tsum, 1, MPI_DOUBLE_PRECISION, &
               MPI_SUM, MPI_COMM_WORLD, ierr )
!yyy = yyy + RealTime()-s
     RETURN
   END IF

   IF ( .NOT. ALLOCATED(ActiveId) ) &
     ALLOCATE( ActiveId(0:ParEnv % PEs-1) )

   nActive = 0
   DO i=0,ParEnv % PEs-1
     IF ( Parenv % Active(i+1) ) THEN
       IF ( i==ParEnv % MyPE ) myid=nActive
       ActiveId(nActive) = i
       nActive = nActive + 1
     END IF
   END DO

   n = 0
   DO WHILE( 2**n<nActive )
     n = n + 1
   END DO

   twop = 1
   DO i=1,n
     IF ( MOD(myid,2*twop)==0 .AND. myid+twop<nActive ) THEN
       CALL MPI_RECV(ssum, 1, MPI_DOUBLE_PRECISION, &
          ActiveId(myid+twop), 7000, MPI_COMM_WORLD, status, ierr)
       tsum = tsum + ssum
     ELSE
       CALL MPI_BSEND(tsum, 1, MPI_DOUBLE_PRECISION, &
          ActiveId(myid-twop), 7000, MPI_COMM_WORLD, status, ierr)
       EXIT        
     END IF
     twop = twop*2
   END DO

   twop = 2**(n-1)
   DO i=n,1,-1
     IF ( MOD(myid,2*twop)==0 .AND. myid+twop<nActive ) THEN
       CALL MPI_BSEND(tsum, 1, MPI_DOUBLE_PRECISION, &
          ActiveId(myid+twop), 7001, MPI_COMM_WORLD, status, ierr)
     ELSE IF ( MOD(myid,twop)==0 ) THEN
       CALL MPI_RECV(tsum, 1, MPI_DOUBLE_PRECISION, &
          ActiveId(myid-twop), 7001, MPI_COMM_WORLD, status, ierr)
     END IF
     twop = twop/2
   END DO
!yyy = yyy + RealTime()-s
!*********************************************************************
END SUBROUTINE SParActiveSUM
!*********************************************************************


!*********************************************************************
!*********************************************************************
!
! Compute global dot product of vectors x and y
!
!*********************************************************************
FUNCTION SParDotProd( ndim, x, xind, y, yind ) RESULT(dres)
!*********************************************************************
  IMPLICIT NONE

  ! Parameters

  INTEGER :: ndim, xind, yind
  REAL(KIND=dp) :: x(*)
  REAL(KIND=dp) :: y(*)
  REAL(KIND=dp) :: dres

  ! Local variables

  REAL(KIND=dp) :: s, realtime
  INTEGER :: i

  !*********************************************************************
   dres = 0
   DO i = 1, ndim
      dres = dres + y(i) * x(i)
   END DO
   CALL SParActiveSUM(dres)
!*********************************************************************
END FUNCTION SParDotProd
!*********************************************************************


!*********************************************************************
!*********************************************************************
!
! Compute global 2-norm of vector x
!
FUNCTION SParNorm( ndim, x, xind ) RESULT(dres)
  IMPLICIT NONE

  ! Parameters

  INTEGER :: ndim, xind
  REAL(KIND=dp) :: x(*)
  REAL(KIND=dp) :: dres

  ! Local variables
  INTEGER :: i
  !*********************************************************************
  dres = 0
  DO i = 1, ndim
    dres = dres + x(i)*x(i)
  END DO
  CALL SParActiveSUM(dres)
  dres = SQRT(dres)
!*********************************************************************
END FUNCTION SParNorm
!*********************************************************************


!*********************************************************************
!*********************************************************************
!
! Compute global dot product of vectors x and y
!
FUNCTION SParCDotProd( ndim, x, xind, y, yind ) result (dres)

  IMPLICIT NONE

  ! Parameters

  INTEGER :: ndim, xind, yind
  COMPLEX(KIND=dp) :: x(*)
  COMPLEX(KIND=dp) :: y(*)
  COMPLEX(KIND=dp) :: dres


  ! Local variables

  COMPLEX(KIND=dp) :: dsum
  INTEGER :: ierr, i, MinActive
  INTEGER, DIMENSION(MPI_STATUS_SIZE) :: status

  !*********************************************************************
  dres = 0.0d0
  IF ( xind == 1 .AND. yind  == 1 ) THEN
     DO i = 1, ndim
        dres = dres + x(i) * y(i)
     END DO
  ELSE
     CALL Fatal( 'SParCDotProd', 'xind or yind not 1' )
  END IF

  IF ( COUNT(Parenv % Active)==ParEnv % PEs ) THEN
    dsum = dres
    CALL MPI_ALLREDUCE( dsum, dres, 1, MPI_DOUBLE_COMPLEX, &
              MPI_SUM, MPI_COMM_WORLD, ierr )
  ELSE
    DO MinActive=0,ParEnv % PEs-1
      IF ( ParEnv % Active(MinActive) ) EXIT
    END DO
    IF ( Parenv % mype == MinActive ) Then
      DO i = 1,COUNT(ParEnv % Active)-1
         CALL MPI_RECV( dsum, 1, MPI_DOUBLE_COMPLEX, &
            MPI_ANY_SOURCE, 7000, MPI_COMM_WORLD, status, ierr )
         dres = dres + dsum
      END DO
      DO i = 1,ParEnv % PEs
         IF ( ParEnv % Active(i) .AND. i-1 /= ParEnv % MyPe ) THEN
            CALL MPI_BSEND( dres, 1, MPI_DOUBLE_COMPLEX, &
               i-1, 7001, MPI_COMM_WORLD, status, ierr )
         END IF
      END DO
    ELSE
      CALL MPI_BSEND( dres, 1, MPI_DOUBLE_COMPLEX, &
            MinActive, 7000, MPI_COMM_WORLD, ierr )
      CALL MPI_RECV( dres, 1, MPI_DOUBLE_COMPLEX, &
            MinActive, 7001, MPI_COMM_WORLD, status, ierr )
    END IF
  END IF
!*********************************************************************
END FUNCTION SParCDotProd
!*********************************************************************


!*********************************************************************
!*********************************************************************
!
! Compute global 2-norm of vector x
!
FUNCTION SParCNorm( ndim, x, xind ) result (norm)
  IMPLICIT NONE

  ! Parameters

  INTEGER :: ndim, xind
  REAL(KIND=dp) :: norm
  COMPLEX(KIND=dp) :: x(*)

  ! Local variables
  INTEGER :: i

  !*********************************************************************
  norm = 0.0d0
  DO i = 1, ndim
     norm = norm + DREAL(x(i))**2 + AIMAG(x(i))**2
  END DO
  CALL SparActiveSUM(norm)
  norm = SQRT(norm)
!*********************************************************************
END FUNCTION SParCNorm
!*********************************************************************



!*********************************************************************
!*********************************************************************
!
! Finalize MPI environment
!
SUBROUTINE ParEnvFinalize()
  IMPLICIT NONE

  ! local variables

  INTEGER :: ierr

  !*********************************************************************
  CALL MPI_BARRIER( MPI_COMM_WORLD, ierr )
  CALL MPI_FINALIZE( ierr )

  IF ( ierr /= 0 ) THEN
     WRITE( Message, * ) 'MPI Finalization failed ! (ierr=', ierr, ')'
     CALL Fatal( 'ParEnvFinalize', Message )
  END IF
!*********************************************************************
END SUBROUTINE ParEnvFinalize
!*********************************************************************



!*********************************************************************
!*********************************************************************
!
! Send parts of the result vector to neighbours
!
!
!*********************************************************************
SUBROUTINE ExchangeResult( SourceMatrix, SplittedMatrix, ParallelInfo, XVec )
  USE types
  IMPLICIT NONE

  TYPE(SplittedMatrixT) :: SplittedMatrix
  TYPE(Matrix_t) :: SourceMatrix
  TYPE (ParallelInfo_t) :: ParallelInfo
  REAL(KIND=dp), DIMENSION(:) :: XVec

  ! Local variables

  INTEGER :: i, j, k, ierr, sproc, destproc, BufLen, ResInd, DOF,TotalSize
  INTEGER, DIMENSION(:), ALLOCATABLE :: IndBuf
  TYPE (ResBufferT), POINTER :: CurrRBuf
  INTEGER, DIMENSION(MPI_STATUS_SIZE) :: status
  REAL(KIND=dp), DIMENSION(:), ALLOCATABLE :: ValBuf

  !*********************************************************************
  TotalSize = 0
  DO i = 1, ParEnv % PEs
     IF ( ParEnv % IsNeighbour(i) ) THEN
       CurrRBuf => SplittedMatrix % ResBuf(i)
       IF ( .NOT. ASSOCIATED(CurrRBuf % ResInd) ) THEN
           TotalSize = TotalSize + 4
       ELSE
           BufLen = SIZE( CurrRBuf % ResInd )
           TotalSize = TotalSize + 4 + 12*Buflen
       END IF
     END IF
  END DO
  TotalSize = 1.5 * TotalSize
  CALL CheckBuffer( TotalSize )

  DO i = 1, ParEnv % PEs
     IF ( ParEnv % IsNeighbour(i) ) THEN

        destproc = i - 1

        CurrRBuf => SplittedMatrix % ResBuf(i)
        IF ( .NOT. ASSOCIATED(CurrRBuf % ResInd) ) THEN
           CALL MPI_BSEND( 0, 1, MPI_INTEGER, destproc, 9000, MPI_COMM_WORLD, ierr )
        ELSE
           BufLen = SIZE( CurrRBuf % ResInd )
           CALL MPI_BSEND( BufLen, 1, MPI_INTEGER, destproc, 9000, MPI_COMM_WORLD, ierr )

           CALL MPI_BSEND( CurrRBuf % ResInd, BufLen, MPI_INTEGER, &
                 destproc, 9001, MPI_COMM_WORLD, ierr )

           CALL MPI_BSEND( CurrRBuf % ResVal, BufLen, MPI_DOUBLE_PRECISION, &
                   destproc, 9002, MPI_COMM_WORLD, ierr )
        END IF

     END IF
  END DO


  DO i = 1, ParEnv % NumOfNeighbours
     CALL MPI_RECV( BufLen, 1, MPI_INTEGER, &
         MPI_ANY_SOURCE, 9000, MPI_COMM_WORLD, status, ierr )

     IF ( BufLen > 0 ) THEN
        sproc = status(MPI_SOURCE)
        ALLOCATE( IndBuf( BufLen ), ValBuf( BufLen ) )

        CALL MPI_RECV( IndBuf, BufLen, MPI_INTEGER, &
             sproc, 9001, MPI_COMM_WORLD, status, ierr )

        CALL MPI_RECV( ValBuf, BufLen, MPI_DOUBLE_PRECISION, &
             sproc, 9002, MPI_COMM_WORLD, status, ierr )

        DO j = 1, BufLen
           ResInd = SearchNode( ParallelInfo, IndBuf(j) )
           IF ( ResInd > 0 ) THEN
              ResInd = SourceMatrix % Perm(ResInd)
              IF ( ResInd > 0 ) XVec(ResInd) = ValBuf(j)
           ELSE
!             WRITE( Message, * ) ParEnv % MyPE, 'Result Receive error: '
!             CALL Fatal( 'ExchangeResult', Message )
           END IF
        END DO

        DEALLOCATE( IndBuf, ValBuf )
     END IF
  END DO
  CALL SparIterActiveBarrier
!*********************************************************************
END SUBROUTINE ExchangeResult
!*********************************************************************



!*********************************************************************
!*********************************************************************
!
! Search an element QueriedNode from an ordered set Nodes and return
! Index to Nodes structure. Return value -1 means QueriedNode was
! not found.
!
FUNCTION SearchNode( ParallelInfo, QueriedNode, First, Last ) RESULT ( Index )

  USE Types
  IMPLICIT NONE

  TYPE (ParallelInfo_t) :: ParallelInfo
  INTEGER :: QueriedNode, Index
  INTEGER, OPTIONAL :: First,Last

  ! Local variables

  INTEGER :: Lower, Upper, Lou, i

  !*********************************************************************

  Index = -1
  Upper = SIZE(ParallelInfo % GlobalDOFs)
  Lower = 1
  IF ( PRESENT( Last  ) ) Upper = Last
  IF ( PRESENT( First ) ) Lower = First

  ! Handle the special case

  IF ( Upper == 0 ) RETURN

10 CONTINUE
  IF ( ParallelInfo % GlobalDOFs(Lower) == QueriedNode ) THEN
     Index = Lower
     RETURN
  ELSE IF ( ParallelInfo % GlobalDOFs(Upper) == QueriedNode ) THEN
     Index = Upper
     RETURN
  END IF

  IF ( (Upper - Lower) > 1 ) THEN
     Lou = ISHFT((Upper + Lower), -1)
     IF ( ParallelInfo % GlobalDOFs(Lou) < QueriedNode ) THEN
        Lower = Lou
        GOTO 10
     ELSE
        Upper = Lou
        GOTO 10
     END IF
  END IF

  RETURN
!*********************************************************************
END FUNCTION SearchNode
!*********************************************************************


!*********************************************************************
!*********************************************************************
!
! Search an element Item from an ordered integer array(N) and return
! Index to that array element. Return value -1 means Item was not found.
!
FUNCTION SearchIAItem( N, IArray, Item, SortOrder ) RESULT ( Index )

  USE types
  IMPLICIT NONE

  INTEGER :: Item, Index, i
  INTEGER :: N
  INTEGER, DIMENSION(:) :: IArray
  INTEGER, OPTIONAL :: SortOrder(:)

  ! Local variables

  INTEGER :: Lower, Upper, lou

  !*********************************************************************

  Index = -1
  Upper =  N
  Lower =  1

  ! Handle the special case

  IF ( Upper == 0 ) RETURN

  IF ( .NOT. PRESENT(SortOrder) ) THEN
     Index = SearchIAItemLinear( n,IArray,Item )
     RETURN
  END IF

  DO WHILE( .TRUE. )
     IF ( IArray(Lower) == Item ) THEN
        Index = Lower
        EXIT
     ELSE IF ( IArray(Upper) == Item ) THEN
        Index = Upper
        EXIT
     END IF

     IF ( (Upper - Lower) > 1 ) THEN
        Lou = ISHFT((Upper + Lower), -1)
        IF ( IArray(lou) < Item ) THEN
           Lower = Lou
        ELSE
           Upper = Lou
        END IF
     ELSE
        EXIT
     END IF
  END DO

 IF (Index>0) Index  = SortOrder(Index)
  RETURN
!*********************************************************************
END FUNCTION SearchIAItem
!*********************************************************************


!*********************************************************************
! Search an element Item from an ordered integer array(N) and return
! Index to that array element. Return value -1 means Item was not found.
!
FUNCTION SearchIAItemLinear( N, IArray, Item ) RESULT ( Index )

  USE types
  IMPLICIT NONE

  INTEGER :: N
  INTEGER, DIMENSION(*) :: IArray
  INTEGER :: Item, Index, i

  ! Local variables

  INTEGER :: Lower, Upper, lou
  !*********************************************************************

  Index = -1
  DO i=1,N
     IF ( IArray(i) == Item ) THEN
       Index = i
       RETURN
     END IF
  END DO
!*********************************************************************
END FUNCTION SearchIAItemLinear
!*********************************************************************


END MODULE SParIterComm
